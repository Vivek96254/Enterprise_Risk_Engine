# ğŸ¦ Enterprise Transaction Risk Analytics & Decision Engine

A production-grade, scalable transaction risk analytics system built with Go, PostgreSQL, and Redis. This system ingests real-time and batch transactions, computes risk scores using a configurable rule engine, and serves analytics through a RESTful API.

![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-336791?style=flat&logo=postgresql)
![Redis](https://img.shields.io/badge/Redis-7+-DC382D?style=flat&logo=redis)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## ğŸ“‹ Table of Contents

- [Problem Statement](#-problem-statement)
- [Architecture](#-architecture)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Quick Start](#-quick-start)
- [API Reference](#-api-reference)
- [Database Schema](#-database-schema)
- [Scaling Strategy](#-scaling-strategy)
- [Deployment](#-deployment)
- [Configuration](#-configuration)

## ğŸ¯ Problem Statement

Financial institutions need to process millions of transactions daily while detecting fraudulent or risky activities in real-time. This system provides:

- **Real-time Risk Assessment**: Score transactions as they occur
- **Configurable Rule Engine**: Flexible rules that can be updated without code changes
- **Historical Analytics**: Insights into transaction patterns and risk trends
- **Horizontal Scalability**: Handle traffic spikes during peak hours
- **Audit Trail**: Complete transaction and decision history for compliance

## ğŸ— Architecture

### Hybrid Pipeline Architecture

This system uses a **True Hybrid Architecture** where:
- **Redis Streams** handle fast, real-time scoring (~30ms)
- **Kafka CDC** captures all database changes for analytics, audit, and ML training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              CLIENT LAYER                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Dashboard   â”‚  â”‚  Mobile Apps â”‚  â”‚ Batch Upload â”‚  â”‚  External    â”‚    â”‚
â”‚  â”‚  (React)     â”‚  â”‚              â”‚  â”‚   (CSV)      â”‚  â”‚  Systems     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         API GATEWAY + AUTH                             â”‚  â”‚
â”‚  â”‚  â€¢ JWT Authentication    â€¢ Rate Limiting    â€¢ CORS    â€¢ Logging       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         INGESTION MODULE                               â”‚  â”‚
â”‚  â”‚  â€¢ Validation        â€¢ Idempotency         â€¢ Batch Processing         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚                                       â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                       â”‚                             â”‚                       â”‚
â”‚                       â–¼                             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚       PostgreSQL           â”‚    â”‚        Redis Streams               â”‚  â”‚
â”‚  â”‚  â€¢ Transactions (Part.)    â”‚    â”‚  â€¢ Event Queue (Fast Path)         â”‚  â”‚
â”‚  â”‚  â€¢ Risk Scores             â”‚    â”‚  â€¢ Consumer Groups                 â”‚  â”‚
â”‚  â”‚  â€¢ Audit Logs              â”‚    â”‚  â€¢ Dead Letter Queue               â”‚  â”‚
â”‚  â”‚  â€¢ Geo Locations           â”‚    â”‚  â€¢ Analytics Cache                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚                                      â”‚                      â”‚
â”‚               â”‚ CDC (Debezium)                       â”‚                      â”‚
â”‚               â–¼                                      â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Kafka              â”‚    â”‚     SCORING WORKERS (Fast Path)    â”‚  â”‚
â”‚  â”‚  â€¢ CDC Events Topic        â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â€¢ Audit Trail             â”‚    â”‚  â”‚ Worker 1 â”‚ â”‚ Worker N â”‚  ~30ms  â”‚  â”‚
â”‚  â”‚  â€¢ Event Replay            â”‚    â”‚  â”‚ â€¢ Rules  â”‚ â”‚ â€¢ Rules  â”‚         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ â€¢ ML     â”‚ â”‚ â€¢ ML     â”‚         â”‚  â”‚
â”‚               â”‚                    â”‚  â”‚ â€¢ Score  â”‚ â”‚ â€¢ Score  â”‚         â”‚  â”‚
â”‚               â–¼                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚   ANALYTICS PIPELINE       â”‚                     â”‚                      â”‚
â”‚  â”‚   (Kafka Consumer)         â”‚                     â”‚                      â”‚
â”‚  â”‚  â€¢ Real-time Metrics       â”‚                     â”‚                      â”‚
â”‚  â”‚  â€¢ Audit Logging           â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ ML Training Data        â”‚    â”‚           Redis Cache              â”‚  â”‚
â”‚  â”‚  â€¢ Data Lake Sync          â”‚    â”‚  â€¢ Risk Score Cache                â”‚  â”‚
â”‚  â”‚  â€¢ Event Replay            â”‚    â”‚  â€¢ Account Profiles                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â€¢ Rate Limiting                   â”‚  â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚                            ENTERPRISE RISK ENGINE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
FAST PATH (Real-time Scoring):
  API Request â†’ Validation â†’ Redis Stream â†’ Worker â†’ Score â†’ DB â†’ Response
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ~30-50ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CDC PATH (Analytics & Audit):
  DB Change â†’ Debezium â†’ Kafka â†’ Analytics Pipeline â†’ Metrics/Audit/ML
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Async, no duplicate scoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Hybrid?

| Aspect | Redis Streams | Kafka CDC |
|--------|---------------|-----------|
| **Purpose** | Real-time scoring | Analytics & Audit |
| **Latency** | ~30ms | ~100-500ms |
| **Scoring** | âœ… Yes | âŒ No (observes only) |
| **Replay** | Limited | âœ… Full event replay |
| **Audit** | Basic | âœ… Complete trail |
| **ML Training** | âŒ | âœ… Training data |

## âœ¨ Features

### Core Capabilities

| Feature | Description |
|---------|-------------|
| **Real-time Ingestion** | Process transactions via REST API (~150-300ms p95 end-to-end) |
| **Fast-Path Scoring** | Sub-100ms for low-risk transactions (async persistence) |
| **Batch Processing** | Upload up to 1000 transactions per batch |
| **Hybrid Scoring** | Rule Engine + Behavioral Analysis + ML Score (pluggable) |
| **Async Scoring** | Redis Streams for reliable event processing |
| **Rule Engine** | JSON-configurable rules with modern fraud patterns |
| **Risk Analytics** | Daily summaries, account profiles, trend analysis |
| **Audit Trail** | Complete transaction and decision history |
| **Rate Limiting** | Token bucket algorithm, 100 req/min per IP |
| **Backtesting** | Replay historical transactions with new rule sets |
| **A/B Testing** | Experiment with different rule sets, statistical significance |
| **Load Testing** | k6 scripts for smoke, load, stress, and spike testing |

### Risk Scoring Rules

#### Classic Rules
| Rule ID | Description | Score Impact |
|---------|-------------|--------------|
| `RULE_CRITICAL_AMOUNT` | Transaction > $10,000 | +40 (Critical) |
| `RULE_SPIKE_ANOMALY` | Amount > 3Ïƒ from average | +30 (High) |
| `RULE_HIGH_RISK_COUNTRY` | Transaction from high-risk country | +35 (High) |
| `RULE_VELOCITY_BURST` | > 10 transactions/hour | +20 (Medium) |
| `RULE_NEW_LOCATION_HIGH_AMOUNT` | New location + > $1,000 | +25 (Medium) |
| `RULE_LOCATION_HOPPING` | > 3 location changes | +15 (Medium) |
| `RULE_NIGHT_TRANSACTION` | Transaction 12am-5am | +10 (Low) |

#### Modern Fraud Pattern Rules ğŸ”¥
| Rule ID | Description | Score Impact |
|---------|-------------|--------------|
| `RULE_SEQUENCE_EXFIL_PATTERN` | Small probe txn â†’ large txn within 5 min | +35 (High) |
| `RULE_PEER_GROUP_ANOMALY` | User deviates 3Ïƒ from similar accounts | +25 (Medium) |
| `RULE_SHARED_BENEFICIARY_NETWORK` | Multiple accounts sending to same target | +30 (High) |
| `RULE_RAPID_DEVICE_SWITCH` | New device + high amount transaction | +25 (Medium) |
| `RULE_GEO_IMPOSSIBLE_TRAVEL` | Location change faster than flight speed | +40 (Critical) |
| `RULE_RAPID_CHANNEL_SWITCH` | Switching onlineâ†’POSâ†’ATM rapidly | +15 (Medium) |
| `RULE_BEHAVIORAL_ANOMALY` | Composite behavioral score > threshold | +20 (Medium) |

### Risk Levels

| Level | Score Range | Action |
|-------|-------------|--------|
| Low | 0-24 | Processed |
| Medium | 25-49 | Processed |
| High | 50-69 | Flagged |
| Critical | 70-100 | Blocked |

### Hybrid Scoring Architecture ğŸ§ 

The system uses a modern **hybrid scoring model** combining multiple signal sources:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYBRID RISK SCORING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Transaction                                                   â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚              FEATURE COMPUTATION                         â”‚  â”‚
â”‚   â”‚  â€¢ Spending patterns (rolling avg, std dev)             â”‚  â”‚
â”‚   â”‚  â€¢ Velocity metrics (tx/hour, z-scores)                 â”‚  â”‚
â”‚   â”‚  â€¢ Location analysis (distance, impossible travel)      â”‚  â”‚
â”‚   â”‚  â€¢ Peer group comparison                                â”‚  â”‚
â”‚   â”‚  â€¢ Sequence detection (probe patterns)                  â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚       â–¼              â–¼              â–¼              â”‚           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚           â”‚
â”‚   â”‚ RULE   â”‚    â”‚BEHAVIORâ”‚    â”‚   ML   â”‚          â”‚           â”‚
â”‚   â”‚ ENGINE â”‚    â”‚ANALYSISâ”‚    â”‚ SCORER â”‚          â”‚           â”‚
â”‚   â”‚  50%   â”‚    â”‚  35%   â”‚    â”‚  15%   â”‚          â”‚           â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚           â”‚
â”‚       â”‚             â”‚             â”‚               â”‚           â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚           â”‚
â”‚                     â”‚                             â”‚           â”‚
â”‚                     â–¼                             â”‚           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚              FINAL SCORE = Î£(weight Ã— score)            â”‚  â”‚
â”‚   â”‚                                                         â”‚  â”‚
â”‚   â”‚  Final = (0.50 Ã— RuleScore) +                          â”‚  â”‚
â”‚   â”‚          (0.35 Ã— BehavioralScore) +                    â”‚  â”‚
â”‚   â”‚          (0.15 Ã— MLScore)                              â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Score Breakdown (stored with each transaction):**
```json
{
  "score": 42.5,
  "rule_score": 35.0,
  "behavioral_score": 55.0,
  "ml_score": 48.0,
  "anomalies_detected": ["SPENDING_SPIKE", "PEER_GROUP_DEVIATION"],
  "scoring_path": "full"
}
```

**ML Integration (Pluggable):**
- Current: Lightweight behavioral z-score ensemble
- Future: External ML service (SageMaker, Vertex AI, custom)
- Interface: `MLScorerInterface` for easy swapping

### Fast-Path Scoring âš¡

For low-risk transactions, the system supports fast-path scoring:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FAST-PATH OPTIMIZATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Transaction arrives                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚ Quick Risk Checkâ”‚  (in-memory, ~10ms)                      â”‚
â”‚   â”‚ â€¢ Amount < $500 â”‚                                          â”‚
â”‚   â”‚ â€¢ Known device  â”‚                                          â”‚
â”‚   â”‚ â€¢ Normal hours  â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚            â”‚                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚    â”‚               â”‚                                           â”‚
â”‚    â–¼               â–¼                                           â”‚
â”‚ LOW RISK        HIGH RISK                                      â”‚
â”‚    â”‚               â”‚                                           â”‚
â”‚    â–¼               â–¼                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚ â”‚FAST PATH â”‚  â”‚   FULL PIPELINE  â”‚                            â”‚
â”‚ â”‚â€¢ Score   â”‚  â”‚â€¢ All rules       â”‚                            â”‚
â”‚ â”‚  inline  â”‚  â”‚â€¢ ML scoring      â”‚                            â”‚
â”‚ â”‚â€¢ Return  â”‚  â”‚â€¢ Behavioral      â”‚                            â”‚
â”‚ â”‚  <100ms  â”‚  â”‚â€¢ ~150-300ms      â”‚                            â”‚
â”‚ â”‚â€¢ Async   â”‚  â”‚                  â”‚                            â”‚
â”‚ â”‚  persist â”‚  â”‚                  â”‚                            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›  Tech Stack

- **Language**: Go 1.21+
- **Web Framework**: Gin
- **Database**: PostgreSQL 15+ (with table partitioning)
- **Message Queue**: Redis Streams
- **Cache**: Redis
- **Auth**: JWT (HS256)
- **Container**: Docker & Docker Compose

## ğŸš€ Quick Start

### Prerequisites

- Go 1.21+
- Docker & Docker Compose
- Make (optional)

### Local Development

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/enterprise-risk-engine.git
cd enterprise-risk-engine
```

2. **Start infrastructure**
```bash
docker-compose up -d postgres redis
```

3. **Run database migrations**
```bash
psql $DATABASE_URL -f db/migrations/001_initial_schema.sql
psql $DATABASE_URL -f db/migrations/002_create_partitions.sql
psql $DATABASE_URL -f db/migrations/003_seed_rules.sql
```

4. **Start the API server**
```bash
cp configs/env.example .env
go run ./cmd/api-server
```

5. **Start the worker (in another terminal)**
```bash
go run ./cmd/worker
```

### Using Docker Compose (Recommended)

```bash
# Start all services (API, Workers, Dashboard)
docker-compose up -d

# View logs
docker-compose logs -f

# Scale workers
docker-compose --profile scale up -d
```

### Dashboard

The system includes a sleek, minimalistic dashboard for real-time monitoring:

```bash
# With Docker (recommended) - Dashboard available at http://localhost:3000
docker-compose up -d

# Or serve locally with Python (API must be running on :8080)
make dashboard
```

**Dashboard Features:**
- ğŸ“Š Real-time system metrics (TPS, latency, error rate)
- ğŸ“ˆ Risk distribution visualization
- ğŸš© Flagged transactions view with rule details
- ğŸ§ª A/B Testing experiment management
- ğŸ” Transaction search by account
- ğŸŒ™ Beautiful dark theme with smooth animations

![Dashboard Preview](docs/dashboard-preview.png)

# Stop all services
docker-compose down
```

## ğŸ“¡ API Reference

### Authentication

#### Register User
```bash
POST /api/v1/auth/register
Content-Type: application/json

{
  "email": "user@example.com",
  "password": "SecurePass123",
  "role": "user"
}
```

#### Login
```bash
POST /api/v1/auth/login
Content-Type: application/json

{
  "email": "user@example.com",
  "password": "SecurePass123"
}
```

**Response:**
```json
{
  "token": "eyJhbGciOiJIUzI1NiIs...",
  "expires_in": 86400,
  "user": {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "email": "user@example.com",
    "role": "user"
  }
}
```

### Transactions

#### Ingest Transaction
```bash
POST /api/v1/transactions
Authorization: Bearer <token>
Content-Type: application/json

{
  "account_id": "550e8400-e29b-41d4-a716-446655440000",
  "amount": 1500.00,
  "currency": "USD",
  "merchant": "Amazon",
  "merchant_category": "retail",
  "location": "New York, NY",
  "country": "US",
  "channel": "online",
  "idempotency_key": "tx-unique-key-123"
}
```

#### Batch Ingest
```bash
POST /api/v1/transactions/batch
Authorization: Bearer <token>
Content-Type: application/json

{
  "transactions": [
    { "account_id": "...", "amount": 100, ... },
    { "account_id": "...", "amount": 200, ... }
  ]
}
```

#### Get Flagged Transactions
```bash
GET /api/v1/transactions/flagged?page=1&page_size=20
Authorization: Bearer <token>
```

### Risk Analytics

#### Get Risk Summary
```bash
GET /api/v1/risk/summary?date=2026-02-03
Authorization: Bearer <token>
```

**Response:**
```json
{
  "date": "2026-02-03",
  "total_transactions": 15420,
  "total_amount": 2543210.50,
  "flagged_count": 234,
  "blocked_count": 45,
  "avg_risk_score": 18.5,
  "high_risk_count": 156,
  "critical_risk_count": 45,
  "top_rules_triggered": [
    {"rule_id": "RULE_VELOCITY_BURST", "count": 89},
    {"rule_id": "RULE_SPIKE_ANOMALY", "count": 67}
  ]
}
```

#### Get Account Risk Profile
```bash
GET /api/v1/risk/account/{account_id}
Authorization: Bearer <token>
```

### System Metrics
```bash
GET /api/v1/metrics/system
Authorization: Bearer <token>  # Requires admin/analyst role
```

**Response:**
```json
{
  "timestamp": "2026-02-03T10:30:00Z",
  "transactions_per_sec": 125.5,
  "avg_processing_time_ms": 45.2,
  "queue_depth": 150,
  "active_workers": 5,
  "db_connections_active": 12,
  "db_connections_idle": 13,
  "error_rate": 0.002
}
```

### Backtesting (Event Replay)
```bash
POST /api/v1/backtest/run
Authorization: Bearer <token>  # Requires admin/analyst role
Content-Type: application/json

{
  "account_id": "550e8400-e29b-41d4-a716-446655440000",
  "start_date": "2026-01-01T00:00:00Z",
  "end_date": "2026-02-01T00:00:00Z",
  "sample_size": 100
}
```

**Response:**
```json
{
  "total_transactions": 100,
  "processed_count": 98,
  "failed_count": 2,
  "average_score": 22.5,
  "risk_distribution": {
    "low": 65,
    "medium": 20,
    "high": 10,
    "critical": 3
  },
  "top_triggered_rules": [
    {"rule_id": "RULE_VELOCITY_BURST", "count": 25},
    {"rule_id": "RULE_NEW_LOCATION_HIGH_AMOUNT", "count": 18}
  ],
  "processing_time_ms": 1250,
  "comparison_with_live": {
    "matching_scores": 85,
    "different_scores": 13,
    "avg_score_difference": 2.3,
    "upgraded_risk": 8,
    "downgraded_risk": 5
  }
}
```

### A/B Testing (Experiments)

#### Create Experiment
```bash
POST /api/v1/experiments
Authorization: Bearer <token>  # Requires admin role
Content-Type: application/json

{
  "name": "New Velocity Rules Test",
  "description": "Testing stricter velocity rules",
  "control_rules": ["RULE_VELOCITY_BURST", "RULE_SPIKE_ANOMALY"],
  "test_rules": ["RULE_VELOCITY_BURST", "RULE_SPIKE_ANOMALY", "RULE_RAPID_SMALL_TRANSACTIONS"],
  "traffic_split": 0.2
}
```

#### Start Experiment
```bash
POST /api/v1/experiments/{id}/start
Authorization: Bearer <token>
```

#### Get Experiment Results
```bash
GET /api/v1/experiments/{id}/results
Authorization: Bearer <token>
```

**Response:**
```json
{
  "experiment_id": "abc123",
  "control": {
    "total_transactions": 800,
    "total_amount": 125000.50,
    "avg_risk_score": 18.5,
    "risk_distribution": {"low": 650, "medium": 100, "high": 40, "critical": 10},
    "flagged_count": 40,
    "blocked_count": 10,
    "rules_triggered": {"RULE_VELOCITY_BURST": 45, "RULE_SPIKE_ANOMALY": 30}
  },
  "test": {
    "total_transactions": 200,
    "total_amount": 31250.25,
    "avg_risk_score": 24.2,
    "risk_distribution": {"low": 140, "medium": 35, "high": 18, "critical": 7},
    "flagged_count": 18,
    "blocked_count": 7,
    "rules_triggered": {"RULE_VELOCITY_BURST": 15, "RULE_RAPID_SMALL_TRANSACTIONS": 22}
  },
  "start_time": "2026-02-01T00:00:00Z",
  "last_updated": "2026-02-03T10:30:00Z"
}
```

#### Get Statistical Significance
```bash
GET /api/v1/experiments/{id}/significance
Authorization: Bearer <token>
```

**Response:**
```json
{
  "is_significant": true,
  "confidence_level": 0.95,
  "p_value": 0.023,
  "score_difference": 5.7,
  "score_difference_pct": 30.8,
  "flag_rate_difference": 0.025,
  "sample_size_control": 800,
  "sample_size_test": 200,
  "recommendation": "Test group shows 30.8% higher risk scores. Consider if this aligns with your goals."
}
```

#### Stop/Pause/Delete Experiment
```bash
POST /api/v1/experiments/{id}/stop
POST /api/v1/experiments/{id}/pause
DELETE /api/v1/experiments/{id}
```

## ğŸ§ª Load Testing

Run load tests using k6:

```bash
# Install k6
brew install k6  # macOS
# or: sudo apt install k6  # Ubuntu

# Run smoke test (quick validation)
k6 run scripts/load_test.js

# Run with custom VUs and duration
k6 run --vus 50 --duration 5m scripts/load_test.js

# Run against production
k6 run --env BASE_URL=https://your-api.onrender.com scripts/load_test.js
```

**Test Scenarios:**
| Scenario | VUs | Duration | Purpose |
|----------|-----|----------|---------|
| smoke | 5 | 30s | Quick validation |
| load | 50 | 5m | Normal load testing |
| stress | 100-200 | 10m | Find breaking point |
| spike | 10â†’200â†’10 | 2m | Sudden traffic spike |

## ğŸ“Š Database Schema

### Entity Relationship Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    users     â”‚       â”‚   accounts   â”‚       â”‚    transactions      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚   (partitioned)      â”‚
â”‚ id (PK)      â”‚â”€â”€â”    â”‚ id (PK)      â”‚â”€â”€â”    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ email        â”‚  â”‚    â”‚ user_id (FK) â”‚â—„â”€â”˜    â”‚ id (PK)              â”‚
â”‚ password_hashâ”‚  â””â”€â”€â”€â–ºâ”‚ account_type â”‚       â”‚ account_id (FK)      â”‚â—„â”€â”
â”‚ role         â”‚       â”‚ risk_profile â”‚       â”‚ amount               â”‚  â”‚
â”‚ created_at   â”‚       â”‚ status       â”‚       â”‚ currency             â”‚  â”‚
â”‚ updated_at   â”‚       â”‚ created_at   â”‚       â”‚ merchant             â”‚  â”‚
â”‚ deleted_at   â”‚       â”‚ updated_at   â”‚       â”‚ location             â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ country              â”‚  â”‚
                                              â”‚ channel              â”‚  â”‚
                                              â”‚ status               â”‚  â”‚
                                              â”‚ idempotency_key      â”‚  â”‚
                                              â”‚ created_at           â”‚  â”‚
                                              â”‚ processed_at         â”‚  â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    risk_scores       â”‚       â”‚    audit_logs        â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚ id (PK)              â”‚       â”‚ id (PK)              â”‚                â”‚
â”‚ transaction_id (FK)  â”‚â”€â”€â”€â”€â”€â”€â”€â”‚ event_type           â”‚                â”‚
â”‚ score                â”‚       â”‚ entity_id            â”‚                â”‚
â”‚ risk_level           â”‚       â”‚ entity_type          â”‚                â”‚
â”‚ rules_triggered[]    â”‚       â”‚ user_id (FK)         â”‚                â”‚
â”‚ features (JSONB)     â”‚       â”‚ action               â”‚                â”‚
â”‚ model_version        â”‚       â”‚ payload (JSONB)      â”‚                â”‚
â”‚ processing_time_ms   â”‚       â”‚ ip_address           â”‚                â”‚
â”‚ created_at           â”‚       â”‚ request_id           â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ created_at           â”‚                â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚      rules           â”‚                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                               â”‚
â”‚ id (PK)              â”‚                                               â”‚
â”‚ name                 â”‚                                               â”‚
â”‚ description          â”‚                                               â”‚
â”‚ condition (JSONB)    â”‚                                               â”‚
â”‚ score_impact         â”‚                                               â”‚
â”‚ risk_level           â”‚                                               â”‚
â”‚ priority             â”‚                                               â”‚
â”‚ enabled              â”‚                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
```

### Table Partitioning

Transactions are partitioned by month for optimal query performance:

```sql
transactions_2026_01  -- January 2026
transactions_2026_02  -- February 2026
...
transactions_2026_12  -- December 2026
```

## ğŸ“ˆ Scaling Strategy

### Horizontal Scaling

| Component | Scaling Method | Notes |
|-----------|---------------|-------|
| API Servers | Add replicas behind load balancer | Stateless, scale independently |
| Workers | Add more worker instances | Consumer groups ensure no duplicate processing |
| PostgreSQL | Read replicas for analytics | Write to primary, read from replicas |
| Redis | Redis Cluster for high availability | Streams support consumer groups natively |

### Performance Optimizations

1. **Database**
   - Monthly partitioning on transactions table
   - Composite indexes on (account_id, created_at)
   - Batch inserts for high-throughput ingestion
   - Connection pooling with pgx

2. **Queue Processing**
   - Consumer groups for parallel processing
   - Batch acknowledgment
   - Dead letter queue for failed messages
   - Automatic retry with exponential backoff

3. **Caching**
   - Risk scores cached for 24 hours
   - Account profiles cached for 5 minutes
   - Daily summaries cached (longer for historical data)

### Load Handling

```
Normal Load:    1 API + 1 Worker  â†’  ~100 TPS
Medium Load:    2 API + 3 Workers â†’  ~500 TPS
High Load:      4 API + 10 Workers â†’ ~2000 TPS
```

## ğŸ¯ Design Tradeoffs

### Why Redis Streams over Kafka?

| Factor | Redis Streams | Kafka |
|--------|--------------|-------|
| **Cost** | Free tier friendly (Upstash) | Requires managed service ($100+/mo) |
| **Operational Complexity** | Single binary, minimal config | ZooKeeper/KRaft, partitions, topics |
| **Throughput** | ~10K TPS (sufficient for this scale) | ~100K+ TPS |
| **Consumer Groups** | Native support | Native support |
| **Message Retention** | Memory-bound, configurable | Disk-based, unlimited |

**Decision**: Redis Streams provides Kafka-like semantics (consumer groups, acknowledgments, replay) at zero infrastructure cost. For free-tier deployment, this is the pragmatic choice. Migration to Kafka is straightforward when scale demands it.

### Why Modular Monolith First?

```
Monolith Benefits:
â”œâ”€â”€ Single deployment unit (simpler CI/CD)
â”œâ”€â”€ Shared database transactions
â”œâ”€â”€ No network latency between modules
â”œâ”€â”€ Easier debugging and tracing
â””â”€â”€ Faster development iteration

Microservices Later:
â”œâ”€â”€ When team grows beyond 5-7 engineers
â”œâ”€â”€ When modules need independent scaling
â”œâ”€â”€ When different tech stacks are needed
â””â”€â”€ When deployment independence is critical
```

**Decision**: Start with clear module boundaries (`internal/auth`, `internal/scoring`, etc.) but deploy as one unit. This avoids premature distributed systems complexity while maintaining the option to split later.

### Why Partitioning over Sharding?

| Approach | Partitioning | Sharding |
|----------|-------------|----------|
| **Complexity** | Native PostgreSQL feature | Requires application logic or Citus |
| **Query Routing** | Automatic partition pruning | Manual shard key routing |
| **Transactions** | Full ACID within DB | Distributed transactions needed |
| **Maintenance** | `DETACH PARTITION` for archival | Complex rebalancing |

**Decision**: Time-based partitioning (monthly) handles our access patterns perfectlyâ€”most queries filter by `created_at`. Sharding would be necessary only if single-partition write throughput becomes a bottleneck (unlikely below 50K TPS).

## ğŸ›¡ï¸ Failure Scenarios & Recovery

### Worker Crash Mid-Batch

```
Scenario: Worker processes 50 of 100 messages, then crashes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Messages remain in "pending" state (not acknowledged)   â”‚
â”‚ 2. Redis tracks pending messages per consumer              â”‚
â”‚ 3. Other workers claim abandoned messages after 30s        â”‚
â”‚ 4. XCLAIM moves ownership to healthy worker                â”‚
â”‚ 5. Processing resumes from where it left off               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: Consumer groups + pending entry list (PEL) ensure at-least-once delivery. Idempotency keys prevent duplicate scoring.

### Duplicate Events

```
Scenario: Network glitch causes same transaction to be published twice
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. First event arrives â†’ Transaction created with          â”‚
â”‚    idempotency_key = "tx-abc-123"                          â”‚
â”‚ 2. Second event arrives â†’ INSERT fails (unique constraint) â”‚
â”‚ 3. API returns existing transaction (HTTP 200, not 201)    â”‚
â”‚ 4. No duplicate processing occurs                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: `idempotency_key` column with unique constraint + `ON CONFLICT DO NOTHING` for batch inserts.

### Database Outage

```
Scenario: PostgreSQL becomes unavailable for 2 minutes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server:                                                 â”‚
â”‚ â”œâ”€â”€ Connection pool detects failure                        â”‚
â”‚ â”œâ”€â”€ Returns HTTP 503 (Service Unavailable)                 â”‚
â”‚ â””â”€â”€ Health check fails â†’ Load balancer removes instance    â”‚
â”‚                                                             â”‚
â”‚ Worker:                                                     â”‚
â”‚ â”œâ”€â”€ DB operations fail with timeout                        â”‚
â”‚ â”œâ”€â”€ Messages not acknowledged (remain pending)             â”‚
â”‚ â”œâ”€â”€ Exponential backoff on retries                         â”‚
â”‚ â””â”€â”€ After max retries â†’ Dead letter queue                  â”‚
â”‚                                                             â”‚
â”‚ Recovery:                                                   â”‚
â”‚ â”œâ”€â”€ DB comes back online                                   â”‚
â”‚ â”œâ”€â”€ Connection pool reconnects automatically               â”‚
â”‚ â”œâ”€â”€ Pending messages reprocessed                           â”‚
â”‚ â””â”€â”€ DLQ messages can be replayed manually                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: Connection pooling with health checks, message persistence in Redis, dead letter queue for failed messages.

### Redis Outage

```
Scenario: Redis becomes unavailable
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Impact:                                                     â”‚
â”‚ â”œâ”€â”€ New transactions saved to DB but not queued            â”‚
â”‚ â”œâ”€â”€ Scoring delayed (not lost)                             â”‚
â”‚ â”œâ”€â”€ Cache misses â†’ Direct DB queries (slower)              â”‚
â”‚ â””â”€â”€ Rate limiting falls back to permissive mode            â”‚
â”‚                                                             â”‚
â”‚ Recovery:                                                   â”‚
â”‚ â”œâ”€â”€ Unscored transactions detected via status='pending'    â”‚
â”‚ â”œâ”€â”€ Batch job can requeue pending transactions             â”‚
â”‚ â””â”€â”€ Cache rebuilds on-demand                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: Transactions are persisted to PostgreSQL first, then queued. Redis is used for acceleration, not as source of truth.

## ğŸ” Security Considerations

### Authentication & Authorization

| Layer | Implementation | Details |
|-------|---------------|---------|
| **Password Storage** | bcrypt (cost factor 12) | Resistant to rainbow tables, GPU attacks |
| **Token Format** | JWT (HS256) | Stateless, includes user ID and role |
| **Token Expiration** | 24 hours | Configurable via `JWT_EXPIRATION` |
| **Role-Based Access** | admin, analyst, user | Middleware enforces per-endpoint |

### JWT Security Best Practices

```go
// Current implementation
Token: {
  "user_id": "uuid",
  "email": "user@example.com",
  "role": "user",
  "exp": 1706954400,  // 24h expiration
  "iat": 1706868000   // Issued at
}

// Rotation strategy (recommended for production):
// 1. Short-lived access tokens (15 min)
// 2. Long-lived refresh tokens (7 days)
// 3. Refresh token rotation on use
// 4. Token blacklist for logout/revocation
```

### Rate Limiting

```
Current: Token bucket algorithm
â”œâ”€â”€ 100 requests per minute per IP
â”œâ”€â”€ Automatic cleanup of stale entries
â”œâ”€â”€ Returns 429 with Retry-After header
â””â”€â”€ Protects against:
    â”œâ”€â”€ Brute force login attempts
    â”œâ”€â”€ API abuse / scraping
    â””â”€â”€ DoS attacks (basic)

Production Enhancements:
â”œâ”€â”€ Per-user rate limits (not just IP)
â”œâ”€â”€ Tiered limits by role/plan
â”œâ”€â”€ Distributed rate limiting (Redis)
â””â”€â”€ Adaptive limits based on behavior
```

### Data Protection

| Concern | Mitigation |
|---------|-----------|
| **SQL Injection** | Parameterized queries via pgx (prepared statements) |
| **XSS** | JSON API only, no HTML rendering |
| **CSRF** | Stateless JWT auth, no cookies |
| **Data at Rest** | PostgreSQL encryption (Render managed) |
| **Data in Transit** | TLS enforced (Render provides HTTPS) |
| **PII Handling** | Minimal PII stored, audit logs for access |

### Audit Trail Immutability

```sql
-- Audit logs table design
CREATE TABLE audit_logs (
    id UUID PRIMARY KEY,
    event_type VARCHAR(50) NOT NULL,
    entity_id UUID,
    entity_type VARCHAR(50),
    user_id UUID,
    action VARCHAR(50) NOT NULL,
    payload JSONB,              -- Full event details
    ip_address INET,            -- Client IP
    request_id VARCHAR(100),    -- Correlation ID
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    -- No updated_at, no deleted_at â†’ Immutable
);

-- Protection:
-- 1. No UPDATE/DELETE permissions for application user
-- 2. Append-only table design
-- 3. Consider write-once storage (S3 Glacier) for compliance
-- 4. Cryptographic hashing for tamper detection (future)
```

### Security Checklist for Production

- [ ] Rotate `JWT_SECRET` (use 256-bit random key)
- [ ] Enable PostgreSQL SSL (`sslmode=require`)
- [ ] Set up WAF rules (Cloudflare/AWS WAF)
- [ ] Implement IP allowlisting for admin endpoints
- [ ] Add request signing for inter-service calls
- [ ] Set up security headers (HSTS, CSP, etc.)
- [ ] Regular dependency vulnerability scanning
- [ ] Penetration testing before launch

## â˜ï¸ Deployment

### Render.com (Free Tier)

1. **Create services on Render:**
   - Web Service (API Server)
   - Background Worker (Scoring Engine)
   - PostgreSQL (Managed)
   - Redis (Use Upstash for free tier)

2. **Configure environment variables:**
```
DATABASE_URL=<render-postgres-url>
REDIS_URL=<upstash-redis-url>
JWT_SECRET=<generate-secure-key>
ENVIRONMENT=production
```

3. **Deploy using render.yaml:**
```bash
render blueprint apply
```

### Docker Production

```bash
# Build images
docker build -t risk-engine-api -f Dockerfile.api .
docker build -t risk-engine-worker -f Dockerfile.worker .

# Push to registry
docker push your-registry/risk-engine-api:latest
docker push your-registry/risk-engine-worker:latest
```

## âš™ï¸ Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `PORT` | 8080 | API server port |
| `ENVIRONMENT` | development | Environment (development/production) |
| `DATABASE_URL` | - | PostgreSQL connection string |
| `REDIS_URL` | - | Redis connection string |
| `JWT_SECRET` | - | Secret for JWT signing |
| `JWT_EXPIRATION` | 24h | Token expiration duration |
| `WORKER_CONCURRENCY` | 5 | Number of worker goroutines |
| `WORKER_BATCH_SIZE` | 100 | Messages per batch |

## ğŸ“¡ Observability

The system is designed for enterprise-grade observability and monitoring:

### Structured Logging

All logs are JSON-formatted with correlation IDs for distributed tracing:

```json
{
  "level": "info",
  "time": 1706954400,
  "request_id": "req-abc123",
  "transaction_id": "tx-def456",
  "method": "POST",
  "path": "/api/v1/transactions",
  "status": 201,
  "latency_ms": 145,
  "final_score": 42.5,
  "rule_score": 35.0,
  "behavioral_score": 55.0,
  "scoring_path": "full",
  "rules_triggered": ["RULE_VELOCITY_BURST"],
  "anomalies_detected": ["SPENDING_SPIKE"]
}
```

### Key Metrics Exposed

| Metric | Type | Description |
|--------|------|-------------|
| `transactions_per_sec` | Gauge | Current ingestion rate |
| `avg_processing_time_ms` | Histogram | Scoring latency distribution |
| `queue_depth` | Gauge | Pending messages in Redis Stream |
| `error_rate` | Gauge | Failed transactions / total |
| `db_connections_active` | Gauge | PostgreSQL pool utilization |
| `scoring_path_distribution` | Counter | Fast vs full path breakdown |
| `rules_triggered_total` | Counter | Rule trigger frequency |
| `anomalies_detected_total` | Counter | Anomaly type frequency |

### Alerting Thresholds

```yaml
alerts:
  - name: HighLatency
    condition: avg_processing_time_ms > 500 for 5m
    severity: warning
    
  - name: QueueBacklog
    condition: queue_depth > 1000 for 2m
    severity: critical
    
  - name: HighErrorRate
    condition: error_rate > 0.05 for 5m
    severity: critical
    
  - name: ScoringDrift
    condition: avg_risk_score change > 20% in 1h
    severity: warning
```

### OpenTelemetry Ready

The system is designed for distributed tracing integration:

```go
// Trace context propagation
ctx = otel.GetTextMapPropagator().Extract(ctx, carrier)
span := tracer.Start(ctx, "ScoreTransaction")
defer span.End()

// Span attributes
span.SetAttributes(
    attribute.String("transaction.id", txID),
    attribute.Float64("score.final", finalScore),
    attribute.String("scoring.path", scoringPath),
)
```

**Integration Points:**
- Jaeger / Zipkin for distributed tracing
- Prometheus for metrics scraping
- Grafana for dashboards
- PagerDuty / OpsGenie for alerting

### Health Endpoints

| Endpoint | Purpose |
|----------|---------|
| `GET /health` | Load balancer health check |
| `GET /health/ready` | Kubernetes readiness probe |
| `GET /health/live` | Kubernetes liveness probe |
| `GET /api/v1/metrics/system` | Detailed system metrics (auth required) |

## ğŸ”® Future AWS Migration Path

| Current | AWS Equivalent |
|---------|---------------|
| Redis Streams | Amazon MSK (Kafka) |
| Render PostgreSQL | Amazon RDS |
| Docker on Render | Amazon ECS/EKS |
| Basic Metrics | Amazon CloudWatch + Prometheus |
| Manual Scaling | AWS Auto Scaling |
| Behavioral Scoring | Amazon SageMaker |

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

---

Built with â¤ï¸ for enterprise-grade risk management

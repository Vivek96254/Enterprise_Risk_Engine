# ğŸ¦ Enterprise Transaction Risk Analytics & Decision Engine

A production-grade, scalable transaction risk analytics system built with Go, PostgreSQL, and Redis. This system ingests real-time and batch transactions, computes risk scores using a configurable rule engine, and serves analytics through a RESTful API.

![Go Version](https://img.shields.io/badge/Go-1.24+-00ADD8?style=flat&logo=go)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-336791?style=flat&logo=postgresql)
![Redis](https://img.shields.io/badge/Redis-7+-DC382D?style=flat&logo=redis)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## ğŸ“‹ Table of Contents

- [Problem Statement](#-problem-statement)
- [Architecture](#-architecture)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Quick Start](#-quick-start)
- [API Reference](#-api-reference)
- [Database Schema](#-database-schema)
- [Scaling Strategy](#-scaling-strategy)
- [Deployment](#-deployment)
- [Configuration](#-configuration)

## ğŸ¯ Problem Statement

Financial institutions need to process millions of transactions daily while detecting fraudulent or risky activities in real-time. This system provides:

- **Real-time Risk Assessment**: Score transactions as they occur
- **Configurable Rule Engine**: Flexible rules that can be updated without code changes
- **Historical Analytics**: Insights into transaction patterns and risk trends
- **Horizontal Scalability**: Handle traffic spikes during peak hours
- **Audit Trail**: Complete transaction and decision history for compliance

## ğŸ— Architecture

### Hybrid Pipeline Architecture

This system uses a **True Hybrid Architecture** that separates concerns for optimal performance:

- **Redis Streams (Fast Path)**: Handles real-time transaction scoring with sub-100ms latency
- **Kafka CDC (Analytics Path)**: Captures all database changes asynchronously for analytics, audit trails, and ML training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              CLIENT LAYER                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Dashboard   â”‚  â”‚  Mobile Apps â”‚  â”‚ Batch Upload â”‚  â”‚  External    â”‚    â”‚
â”‚  â”‚  (React)     â”‚  â”‚              â”‚  â”‚   (CSV)      â”‚  â”‚  Systems     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         API GATEWAY + AUTH                             â”‚  â”‚
â”‚  â”‚  â€¢ JWT Authentication    â€¢ Rate Limiting (100/min/IP)                 â”‚  â”‚
â”‚  â”‚  â€¢ CORS                  â€¢ Request ID Generation                      â”‚  â”‚
â”‚  â”‚  â€¢ Structured Logging    â€¢ Error Handling                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    TRANSACTION INGESTION MODULE                        â”‚  â”‚
â”‚  â”‚  â€¢ Input Validation      â€¢ Idempotency Check (deduplication)          â”‚  â”‚
â”‚  â”‚  â€¢ Account Verification  â€¢ Batch Processing (up to 1000 txns)        â”‚  â”‚
â”‚  â”‚  â€¢ Metadata Enrichment   â€¢ Audit Log Creation                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚                                       â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                       â”‚                             â”‚                       â”‚
â”‚                       â–¼                             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚       PostgreSQL           â”‚    â”‚        Redis Streams               â”‚  â”‚
â”‚  â”‚  â€¢ Transactions (Part.)    â”‚    â”‚  â€¢ Event Queue (Fast Path)         â”‚  â”‚
â”‚  â”‚  â€¢ Risk Scores             â”‚    â”‚  â€¢ Consumer Groups                 â”‚  â”‚
â”‚  â”‚  â€¢ Audit Logs              â”‚    â”‚  â€¢ Dead Letter Queue               â”‚  â”‚
â”‚  â”‚  â€¢ Accounts/Users          â”‚    â”‚  â€¢ Analytics Cache                 â”‚  â”‚
â”‚  â”‚  â€¢ Rules Configuration     â”‚    â”‚  â€¢ Rate Limiting State             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚                                      â”‚                      â”‚
â”‚               â”‚ CDC (Debezium)                       â”‚                      â”‚
â”‚               â”‚ (Optional - for analytics)           â”‚                      â”‚
â”‚               â–¼                                      â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Kafka              â”‚    â”‚     SCORING WORKERS (Fast Path)    â”‚  â”‚
â”‚  â”‚  â€¢ CDC Events Topic        â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â€¢ Audit Trail             â”‚    â”‚  â”‚ Worker 1 â”‚ â”‚ Worker N â”‚         â”‚  â”‚
â”‚  â”‚  â€¢ Event Replay            â”‚    â”‚  â”‚ â€¢ Rules  â”‚ â”‚ â€¢ Rules  â”‚         â”‚  â”‚
â”‚  â”‚  â€¢ ML Training Data        â”‚    â”‚  â”‚ â€¢ ML     â”‚ â”‚ â€¢ ML     â”‚         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ â€¢ Score  â”‚ â”‚ â€¢ Score  â”‚         â”‚  â”‚
â”‚               â”‚                    â”‚  â”‚ â€¢ A/B    â”‚ â”‚ â€¢ A/B    â”‚         â”‚  â”‚
â”‚               â–¼                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  Processing: ~30-150ms per txn     â”‚  â”‚
â”‚  â”‚   ANALYTICS PIPELINE       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚   (Kafka Consumer)         â”‚                     â”‚                      â”‚
â”‚  â”‚  â€¢ Real-time Metrics       â”‚                     â”‚                      â”‚
â”‚  â”‚  â€¢ Audit Logging           â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ ML Training Data        â”‚    â”‚           Redis Cache              â”‚  â”‚
â”‚  â”‚  â€¢ Data Lake Sync          â”‚    â”‚  â€¢ Risk Score Cache (24h TTL)      â”‚  â”‚
â”‚  â”‚  â€¢ Event Replay            â”‚    â”‚  â€¢ Account Profiles (5m TTL)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â€¢ Daily Summaries (varies)        â”‚  â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚                            ENTERPRISE RISK ENGINE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Complete Transaction Flow (Step-by-Step)

#### 1. Transaction Ingestion (API Layer)

```
Client Request
    â”‚
    â–¼
[API Gateway]
    â”‚ â€¢ Rate Limiting (100 req/min per IP)
    â”‚ â€¢ JWT Authentication
    â”‚ â€¢ Request ID Generation
    â”‚
    â–¼
[Ingestion Module]
    â”‚ â€¢ Validate request payload
    â”‚ â€¢ Check idempotency key (prevent duplicates)
    â”‚ â€¢ Verify account exists and is active
    â”‚ â€¢ Create transaction record in PostgreSQL
    â”‚ â€¢ Generate audit log entry
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                     â”‚
    â–¼                                     â–¼
[PostgreSQL]                      [Redis Streams]
    â”‚ â€¢ Transaction saved                â”‚ â€¢ Event published to stream
    â”‚   (status: "pending")              â”‚ â€¢ Consumer group: "scoring-workers"
    â”‚                                     â”‚ â€¢ Message ID returned
    â”‚                                     â”‚
    â”‚                                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ (Async - non-blocking)
                    â–¼
            [Scoring Worker]
```

**Key Points:**
- Transaction is **immediately persisted** to PostgreSQL (source of truth)
- Event is **asynchronously published** to Redis Streams
- API returns **HTTP 201** with transaction ID (scoring happens async)
- Idempotency keys prevent duplicate processing

#### 2. Fast-Path Scoring (Redis Streams â†’ Workers)

```
Redis Stream Message
    â”‚
    â–¼
[Worker Consumes Event]
    â”‚ â€¢ Consumer group ensures no duplicate processing
    â”‚ â€¢ Batch processing (100 messages at a time)
    â”‚
    â–¼
[Fetch Transaction from DB]
    â”‚ â€¢ Load full transaction details
    â”‚ â€¢ Get account information
    â”‚
    â–¼
[Feature Computation]
    â”‚ â€¢ Historical transaction analysis (7d, 30d windows)
    â”‚ â€¢ Velocity metrics (tx/hour, tx/day)
    â”‚ â€¢ Location patterns (distance, impossible travel)
    â”‚ â€¢ Peer group comparison
    â”‚ â€¢ Sequence detection (probe patterns)
    â”‚ â€¢ Behavioral anomaly scores
    â”‚
    â–¼
[Hybrid Scoring]
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                         â”‚
    â–¼                                         â–¼
[Rule Engine]                          [ML/Behavioral]
    â”‚ â€¢ Evaluate 15+ rules                    â”‚ â€¢ Behavioral z-score analysis
    â”‚ â€¢ Calculate rule_score (0-100)          â”‚ â€¢ ML score (if available)
    â”‚ â€¢ Track triggered rules                 â”‚ â€¢ Anomaly detection
    â”‚                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            [Final Score Calculation]
                â”‚
                â”‚ Final = (0.50 Ã— RuleScore) +
                â”‚        (0.35 Ã— BehavioralScore) +
                â”‚        (0.15 Ã— MLScore)
                â”‚
                â–¼
        [Risk Level Determination]
            â”‚ â€¢ Low: 0-24
            â”‚ â€¢ Medium: 25-49
            â”‚ â€¢ High: 50-69
            â”‚ â€¢ Critical: 70-100
            â”‚
            â–¼
    [Transaction Status Update]
        â”‚ â€¢ Processed (low/medium)
        â”‚ â€¢ Flagged (high)
        â”‚ â€¢ Blocked (critical)
        â”‚
        â–¼
[Save Risk Score to DB]
    â”‚ â€¢ Store all score components
    â”‚ â€¢ Store features (JSONB)
    â”‚ â€¢ Store triggered rules
    â”‚ â€¢ Store anomalies detected
    â”‚
    â–¼
[Update Cache]
    â”‚ â€¢ Cache risk score (24h TTL)
    â”‚ â€¢ Update account profile cache
    â”‚
    â–¼
[Acknowledge Message]
    â”‚ â€¢ Mark message as processed
    â”‚ â€¢ Remove from pending list
```

**Processing Time:**
- **Fast Path** (low risk): ~30-50ms
- **Full Path** (high risk): ~150-300ms
- **Average**: ~45ms (p50), ~145ms (p95)

#### 3. CDC Path (Kafka - Optional, for Analytics)

```
PostgreSQL Change
    â”‚
    â–¼
[Debezium CDC Connector]
    â”‚ â€¢ Captures INSERT/UPDATE/DELETE
    â”‚ â€¢ Converts to Kafka events
    â”‚ â€¢ Topic: risk-engine.public.transactions
    â”‚
    â–¼
[Kafka Topic]
    â”‚ â€¢ Persistent storage
    â”‚ â€¢ Event replay capability
    â”‚ â€¢ Multiple consumers supported
    â”‚
    â–¼
[Analytics Pipeline Consumer]
    â”‚ â€¢ Real-time metrics aggregation
    â”‚ â€¢ Audit trail logging
    â”‚ â€¢ ML training data collection
    â”‚ â€¢ Data lake synchronization
    â”‚ â€¢ NO SCORING (observes only)
```

**Why Two Paths?**

| Path | Purpose | Latency | Scoring | Use Case |
|------|---------|---------|---------|----------|
| **Redis Streams** | Real-time scoring | ~30-150ms | âœ… Yes | Transaction decision making |
| **Kafka CDC** | Analytics & audit | ~100-500ms | âŒ No | Compliance, ML training, analytics |

**Key Design Decision:**
- **Scoring happens ONCE** via Redis Streams (fast path)
- **Kafka observes** all changes for analytics (no duplicate scoring)
- This prevents double-scoring while enabling comprehensive audit trails

### Fast-Path Optimization

For low-risk transactions, the system can use an optimized fast path:

```
Transaction Arrives
    â”‚
    â–¼
[Quick Risk Assessment]
    â”‚ â€¢ Amount < $500?
    â”‚ â€¢ Known device/location?
    â”‚ â€¢ Normal business hours?
    â”‚ â€¢ Low velocity?
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚
    â–¼              â–¼              â–¼
LOW RISK      MEDIUM RISK    HIGH RISK
    â”‚              â”‚              â”‚
    â–¼              â–¼              â–¼
[FAST PATH]   [FULL PATH]   [FULL PATH]
â€¢ Minimal      â€¢ All rules   â€¢ All rules
  rules        â€¢ Behavioral  â€¢ Behavioral
â€¢ Inline       â€¢ ML scoring â€¢ ML scoring
â€¢ <100ms       â€¢ ~150ms     â€¢ ~300ms
â€¢ Async        â€¢ Async      â€¢ Async
  persist        persist      persist
```

**Fast Path Criteria:**
- Rule score < 20
- Behavioral score < 15
- No critical anomalies detected
- Known account patterns

### Hybrid Scoring Model Details

The system combines three scoring signals:

1. **Rule Engine (50% weight)**
   - 15+ configurable rules
   - Classic fraud patterns (velocity, amount, location)
   - Modern patterns (sequence detection, peer group, impossible travel)
   - Score range: 0-100

2. **Behavioral Analysis (35% weight)**
   - Z-score based anomaly detection
   - Spending pattern deviation
   - Velocity anomalies
   - Temporal pattern analysis
   - Score range: 0-100

3. **ML Scorer (15% weight)**
   - Pluggable ML model interface
   - Current: Lightweight ensemble model
   - Future: External service (SageMaker, Vertex AI)
   - Score range: 0-100 (nullable if ML unavailable)

**Final Score Formula:**
```python
if ml_score is not None:
    final_score = (0.50 Ã— rule_score) + (0.35 Ã— behavioral_score) + (0.15 Ã— ml_score)
else:
    # Redistribute ML weight: 60% to rules, 40% to behavioral
    final_score = (0.59 Ã— rule_score) + (0.41 Ã— behavioral_score)

final_score = min(final_score, 100)  # Cap at 100
```

**Stored Score Breakdown:**
```json
{
  "score": 42.5,              // Final composite score
  "rule_score": 35.0,          // From rule engine
  "behavioral_score": 55.0,    // From behavioral analysis
  "ml_score": 48.0,           // From ML model (nullable)
  "risk_level": "medium",      // Determined from final score
  "rules_triggered": ["RULE_VELOCITY_BURST", "RULE_SPIKE_ANOMALY"],
  "anomalies_detected": ["SPENDING_SPIKE", "PEER_GROUP_DEVIATION"],
  "scoring_path": "full",      // "fast" or "full"
  "model_version": "v2.0.0-hybrid"
}
```

## âœ¨ Features

### Core Capabilities

| Feature | Description |
|---------|-------------|
| **Real-time Ingestion** | Process transactions via REST API (~150-300ms p95 end-to-end) |
| **Fast-Path Scoring** | Sub-100ms for low-risk transactions (async persistence) |
| **Batch Processing** | Upload up to 1000 transactions per batch |
| **Hybrid Scoring** | Rule Engine + Behavioral Analysis + ML Score (pluggable) |
| **Async Scoring** | Redis Streams for reliable event processing |
| **Rule Engine** | JSON-configurable rules with modern fraud patterns |
| **Risk Analytics** | Daily summaries, account profiles, trend analysis |
| **Audit Trail** | Complete transaction and decision history |
| **Rate Limiting** | Token bucket algorithm, 100 req/min per IP |
| **Backtesting** | Replay historical transactions with new rule sets |
| **A/B Testing** | Experiment with different rule sets, statistical significance |
| **Load Testing** | k6 scripts for smoke, load, stress, and spike testing |

### Risk Scoring Rules

#### Classic Rules
| Rule ID | Description | Score Impact |
|---------|-------------|--------------|
| `RULE_CRITICAL_AMOUNT` | Transaction > $10,000 | +40 (Critical) |
| `RULE_SPIKE_ANOMALY` | Amount > 3Ïƒ from average | +30 (High) |
| `RULE_HIGH_RISK_COUNTRY` | Transaction from high-risk country | +35 (High) |
| `RULE_VELOCITY_BURST` | > 10 transactions/hour | +20 (Medium) |
| `RULE_NEW_LOCATION_HIGH_AMOUNT` | New location + > $1,000 | +25 (Medium) |
| `RULE_LOCATION_HOPPING` | > 3 location changes | +15 (Medium) |
| `RULE_NIGHT_TRANSACTION` | Transaction 12am-5am | +10 (Low) |

#### Modern Fraud Pattern Rules ğŸ”¥
| Rule ID | Description | Score Impact |
|---------|-------------|--------------|
| `RULE_SEQUENCE_EXFIL_PATTERN` | Small probe txn â†’ large txn within 5 min | +35 (High) |
| `RULE_PEER_GROUP_ANOMALY` | User deviates 3Ïƒ from similar accounts | +25 (Medium) |
| `RULE_SHARED_BENEFICIARY_NETWORK` | Multiple accounts sending to same target | +30 (High) |
| `RULE_RAPID_DEVICE_SWITCH` | New device + high amount transaction | +25 (Medium) |
| `RULE_GEO_IMPOSSIBLE_TRAVEL` | Location change faster than flight speed | +40 (Critical) |
| `RULE_RAPID_CHANNEL_SWITCH` | Switching onlineâ†’POSâ†’ATM rapidly | +15 (Medium) |
| `RULE_BEHAVIORAL_ANOMALY` | Composite behavioral score > threshold | +20 (Medium) |

### Risk Levels

| Level | Score Range | Action |
|-------|-------------|--------|
| Low | 0-24 | Processed |
| Medium | 25-49 | Processed |
| High | 50-69 | Flagged |
| Critical | 70-100 | Blocked |

### Feature Computation Process

Before scoring, the system computes 30+ risk features from historical data:

```
Transaction Arrives
    â”‚
    â–¼
[Fetch Historical Data]
    â”‚ â€¢ Last 7 days transactions
    â”‚ â€¢ Last 30 days transactions
    â”‚ â€¢ Last 1 hour transactions
    â”‚ â€¢ Last 24 hours transactions
    â”‚
    â–¼
[Compute Spending Patterns]
    â”‚ â€¢ Rolling average (7d, 30d)
    â”‚ â€¢ Standard deviation (30d)
    â”‚ â€¢ Z-score: (amount - mean) / stddev
    â”‚ â€¢ Amount deviation from baseline
    â”‚
    â–¼
[Compute Velocity Metrics]
    â”‚ â€¢ Transactions per hour (last 1h)
    â”‚ â€¢ Transactions per day (last 24h)
    â”‚ â€¢ Velocity z-score (vs historical)
    â”‚ â€¢ Time since last transaction
    â”‚
    â–¼
[Compute Location Patterns]
    â”‚ â€¢ Unique locations (last 7d)
    â”‚ â€¢ Location change count
    â”‚ â€¢ Distance from last transaction (km)
    â”‚ â€¢ Impossible travel detection (speed > 900 km/h)
    â”‚ â€¢ Is new location? (not seen in 7d)
    â”‚ â€¢ High-risk country check
    â”‚
    â–¼
[Compute Merchant Patterns]
    â”‚ â€¢ Is new merchant? (not seen in 7d)
    â”‚ â€¢ Merchant risk score (historical)
    â”‚ â€¢ Merchant category analysis
    â”‚
    â–¼
[Compute Temporal Patterns]
    â”‚ â€¢ Is unusual hour? (vs user's pattern)
    â”‚ â€¢ Day of week anomaly
    â”‚ â€¢ Time since last transaction (hours)
    â”‚
    â–¼
[Compute Sequence Patterns]
    â”‚ â€¢ Recent small transactions count (last 10 min)
    â”‚ â€¢ Follows probe pattern? (small â†’ large)
    â”‚ â€¢ Shared beneficiary count (mule detection)
    â”‚
    â–¼
[Compute Peer Group Metrics]
    â”‚ â€¢ Peer group average spend
    â”‚ â€¢ Deviation from peer group (z-score)
    â”‚ â€¢ Similar accounts comparison
    â”‚
    â–¼
[Compute Behavioral Anomalies]
    â”‚ â€¢ Composite behavioral anomaly score
    â”‚ â€¢ Anomaly ratio (flagged / total)
    â”‚ â€¢ Channel switch count
    â”‚ â€¢ Device change detection
    â”‚
    â–¼
[Feature Set Complete]
    â”‚ â€¢ 30+ features computed
    â”‚ â€¢ Ready for scoring
```

**Example Feature Values:**
```json
{
  "rolling_avg_spend_7d": 450.00,
  "rolling_avg_spend_30d": 520.00,
  "rolling_std_dev_30d": 150.00,
  "spending_z_score": 2.5,
  "transaction_velocity_1h": 3,
  "transaction_velocity_24h": 15,
  "velocity_z_score": 1.8,
  "unique_locations_7d": 2,
  "location_change_count": 1,
  "is_new_location": true,
  "distance_from_last_tx_km": 250.5,
  "is_new_merchant": false,
  "merchant_risk_score": 12.5,
  "time_since_last_tx_hours": 4.5,
  "is_unusual_hour": false,
  "recent_small_tx_count": 0,
  "follows_probe_pattern": false,
  "peer_group_avg_spend": 480.00,
  "peer_group_deviation": 0.5,
  "behavioral_anomaly_score": 25.0
}
```

### Hybrid Scoring Architecture ğŸ§ 

The system uses a modern **hybrid scoring model** combining multiple signal sources:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYBRID RISK SCORING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Transaction + Features                                        â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚       â”‚                                                     â”‚  â”‚
â”‚       â–¼                                                     â–¼  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”‚   RULE ENGINE     â”‚                          â”‚  A/B TEST CHECK  â”‚
â”‚   â”‚   (50% weight)    â”‚                          â”‚  (if active)      â”‚
â”‚   â”‚                   â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   â”‚ â€¢ 15+ rules       â”‚                                   â”‚
â”‚   â”‚ â€¢ Priority order  â”‚                                   â”‚
â”‚   â”‚ â€¢ Score impact    â”‚                                   â”‚
â”‚   â”‚ â€¢ Rule score:     â”‚                                   â”‚
â”‚   â”‚   0-100           â”‚                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚             â”‚                                             â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                â”‚
â”‚                                â–¼
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚              BEHAVIORAL ANALYSIS (35% weight)           â”‚  â”‚
â”‚   â”‚  â€¢ Z-score based anomaly detection                       â”‚  â”‚
â”‚   â”‚  â€¢ Spending pattern deviation                            â”‚  â”‚
â”‚   â”‚  â€¢ Velocity anomalies                                    â”‚  â”‚
â”‚   â”‚  â€¢ Temporal pattern analysis                             â”‚  â”‚
â”‚   â”‚  â€¢ Behavioral score: 0-100                                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚                                 â”‚
â”‚                                â–¼                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚              ML SCORER (15% weight, optional)           â”‚  â”‚
â”‚   â”‚  â€¢ Pluggable ML model interface                         â”‚  â”‚
â”‚   â”‚  â€¢ Current: Lightweight ensemble                        â”‚  â”‚
â”‚   â”‚  â€¢ Future: External service (SageMaker, Vertex AI)      â”‚  â”‚
â”‚   â”‚  â€¢ ML score: 0-100 (nullable)                           â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚                                 â”‚
â”‚                                â–¼                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚              FINAL SCORE CALCULATION                     â”‚  â”‚
â”‚   â”‚                                                         â”‚  â”‚
â”‚   â”‚  if ml_score is not None:                               â”‚  â”‚
â”‚   â”‚    final = (0.50 Ã— rule_score) +                        â”‚  â”‚
â”‚   â”‚            (0.35 Ã— behavioral_score) +                  â”‚  â”‚
â”‚   â”‚            (0.15 Ã— ml_score)                            â”‚  â”‚
â”‚   â”‚  else:                                                  â”‚  â”‚
â”‚   â”‚    final = (0.59 Ã— rule_score) +                        â”‚  â”‚
â”‚   â”‚            (0.41 Ã— behavioral_score)                     â”‚  â”‚
â”‚   â”‚                                                         â”‚  â”‚
â”‚   â”‚  final = min(final, 100)  // Cap at 100                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Score Breakdown (stored with each transaction):**
```json
{
  "score": 42.5,              // Final composite score (0-100)
  "rule_score": 35.0,          // From rule engine (0-100)
  "behavioral_score": 55.0,    // From behavioral analysis (0-100)
  "ml_score": 48.0,           // From ML model (0-100, nullable)
  "risk_level": "medium",      // low/medium/high/critical
  "rules_triggered": ["RULE_VELOCITY_BURST", "RULE_SPIKE_ANOMALY"],
  "anomalies_detected": ["SPENDING_SPIKE", "PEER_GROUP_DEVIATION"],
  "scoring_path": "full",      // "fast" or "full"
  "model_version": "v2.0.0-hybrid",
  "features": {
    "rolling_avg_spend_30d": 520.0,
    "spending_z_score": 2.5,
    "velocity_z_score": 1.8,
    // ... 30+ features
  }
}
```

**ML Integration (Pluggable):**
- **Current**: Lightweight behavioral z-score ensemble (built-in)
- **Future**: External ML service (SageMaker, Vertex AI, custom)
- **Interface**: `MLScorerInterface` for easy swapping
- **Fallback**: If ML unavailable, weight redistributed to rules (60%) and behavioral (40%)

### A/B Testing Flow

The system supports A/B testing of scoring rules to measure impact:

```
1. Create Experiment
   POST /api/v1/experiments
   {
     "name": "Stricter Velocity Rules",
     "control_rules": ["RULE_VELOCITY_BURST"],
     "test_rules": ["RULE_VELOCITY_BURST", "RULE_RAPID_SMALL_TRANSACTIONS"],
     "traffic_split": 0.2  // 20% to test group
   }

2. Start Experiment
   POST /api/v1/experiments/{id}/start
   â”‚
   â–¼
3. Transaction Scoring (with A/B assignment)
   â”‚
   â”œâ”€â–º Account ID â†’ Consistent Hashing â†’ Group Assignment
   â”‚   â€¢ Same account always in same group
   â”‚   â€¢ Traffic split: 80% control, 20% test
   â”‚
   â”œâ”€â–º Control Group
   â”‚   â€¢ Uses control_rules
   â”‚   â€¢ Standard scoring
   â”‚
   â””â”€â–º Test Group
       â€¢ Uses test_rules
       â€¢ Experimental scoring
       â€¢ Results tracked separately
   â”‚
   â–¼
4. Results Aggregation
   â”‚ â€¢ Control group metrics
   â”‚ â€¢ Test group metrics
   â”‚ â€¢ Statistical significance calculation
   â”‚
   â–¼
5. Analysis
   GET /api/v1/experiments/{id}/results
   GET /api/v1/experiments/{id}/significance
   â”‚
   â””â”€â–º Decision: Keep test rules? Stop experiment?
```

**A/B Testing Features:**
- **Consistent Assignment**: Same account always in same group (via consistent hashing)
- **Traffic Splitting**: Configurable split (e.g., 10%, 20%, 50%)
- **Statistical Significance**: P-value, confidence intervals
- **Real-time Tracking**: Results updated as transactions flow
- **Comparison Metrics**: Score differences, flag rate differences

### Backtesting Flow

Replay historical transactions with new rule sets:

```
1. Submit Backtest Request
   POST /api/v1/backtest/run
   {
     "account_id": "...",
     "start_date": "2026-01-01T00:00:00Z",
     "end_date": "2026-02-01T00:00:00Z",
     "sample_size": 100
   }
   â”‚
   â–¼
2. Fetch Historical Transactions
   â”‚ â€¢ Query transactions in date range
   â”‚ â€¢ Sample if sample_size specified
   â”‚ â€¢ Order by created_at
   â”‚
   â–¼
3. Re-score Each Transaction
   â”‚ For each transaction:
   â”‚   â€¢ Load transaction details
   â”‚   â€¢ Compute features (using historical context)
   â”‚   â€¢ Apply current rule set
   â”‚   â€¢ Calculate new score
   â”‚   â€¢ Compare with original score
   â”‚
   â–¼
4. Aggregate Results
   â”‚ â€¢ Total transactions processed
   â”‚ â€¢ Average score
   â”‚ â€¢ Risk distribution
   â”‚ â€¢ Top triggered rules
   â”‚ â€¢ Comparison with live scores:
   â”‚   - Matching scores count
   â”‚   - Different scores count
   â”‚   - Upgraded risk count
   â”‚   - Downgraded risk count
   â”‚
   â–¼
5. Return Results
   {
     "total_transactions": 100,
     "processed_count": 98,
     "average_score": 22.5,
     "risk_distribution": {...},
     "comparison_with_live": {...}
   }
```

**Backtesting Use Cases:**
- Test new rules before deployment
- Measure impact of rule changes
- Validate rule effectiveness
- Compare scoring models

### Fast-Path Scoring âš¡

For low-risk transactions, the system supports fast-path scoring:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FAST-PATH OPTIMIZATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Transaction arrives                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚ Quick Risk Checkâ”‚  (in-memory, ~10ms)                      â”‚
â”‚   â”‚ â€¢ Amount < $500 â”‚                                          â”‚
â”‚   â”‚ â€¢ Known device  â”‚                                          â”‚
â”‚   â”‚ â€¢ Normal hours  â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚            â”‚                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚    â”‚               â”‚                                           â”‚
â”‚    â–¼               â–¼                                           â”‚
â”‚ LOW RISK        HIGH RISK                                      â”‚
â”‚    â”‚               â”‚                                           â”‚
â”‚    â–¼               â–¼                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚ â”‚FAST PATH â”‚  â”‚   FULL PIPELINE  â”‚                            â”‚
â”‚ â”‚â€¢ Score   â”‚  â”‚â€¢ All rules       â”‚                            â”‚
â”‚ â”‚  inline  â”‚  â”‚â€¢ ML scoring      â”‚                            â”‚
â”‚ â”‚â€¢ Return  â”‚  â”‚â€¢ Behavioral      â”‚                            â”‚
â”‚ â”‚  <100ms  â”‚  â”‚â€¢ ~150-300ms      â”‚                            â”‚
â”‚ â”‚â€¢ Async   â”‚  â”‚                  â”‚                            â”‚
â”‚ â”‚  persist â”‚  â”‚                  â”‚                            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›  Tech Stack

- **Language**: Go 1.24+ (compatible with Go 1.21+)
- **Web Framework**: Gin
- **Database**: PostgreSQL 15+ (with table partitioning)
- **Message Queue**: Redis Streams (fast path), Kafka (optional, for CDC analytics)
- **Cache**: Redis
- **Auth**: JWT (HS256)
- **Container**: Docker & Docker Compose
- **CDC**: Debezium (optional, for Kafka CDC pipeline)

## ğŸš€ Quick Start

### Prerequisites

- **Go 1.24+** (or Go 1.21+ for compatibility)
- **Docker & Docker Compose** (for infrastructure)
- **PostgreSQL 15+** (or use Docker)
- **Redis 7+** (or use Docker)
- **Make** (optional, for convenience commands)

### Option 1: Docker Compose (Recommended for First-Time Setup)

This is the easiest way to get everything running:

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/enterprise-risk-engine.git
cd enterprise-risk-engine

# 2. Start all services (PostgreSQL, Redis, API, Workers, Dashboard)
docker-compose up -d

# 3. Wait for services to be ready (about 10-15 seconds)
docker-compose ps

# 4. Run database migrations
make migrate-docker
# OR manually:
# docker exec -i risk-engine-postgres psql -U postgres -d risk_engine < db/migrations/001_initial_schema.sql
# docker exec -i risk-engine-postgres psql -U postgres -d risk_engine < db/migrations/002_create_partitions.sql
# docker exec -i risk-engine-postgres psql -U postgres -d risk_engine < db/migrations/003_seed_rules.sql

# 5. Verify services are running
curl http://localhost:8080/health
# Should return: {"status":"healthy","timestamp":"..."}

# 6. Access the dashboard
# Open http://localhost:3000 in your browser
```

**Services Available:**
- **API Server**: http://localhost:8080
- **Dashboard**: http://localhost:3000
- **PostgreSQL**: localhost:5437 (mapped from container port 5432)
- **Redis**: localhost:6382 (mapped from container port 6379)

### Option 2: Local Development (For Active Development)

For active development with hot-reload capabilities:

```bash
# 1. Start only infrastructure (PostgreSQL + Redis)
docker-compose up -d postgres redis

# 2. Set up environment variables
cp configs/env.example .env
# Edit .env if needed (defaults work for local Docker setup)

# 3. Run database migrations
export DATABASE_URL="postgres://postgres:postgres@localhost:5437/risk_engine?sslmode=disable"
psql $DATABASE_URL -f db/migrations/001_initial_schema.sql
psql $DATABASE_URL -f db/migrations/002_create_partitions.sql
psql $DATABASE_URL -f db/migrations/003_seed_rules.sql

# 4. Start API server (Terminal 1)
go run ./cmd/api-server
# API will be available at http://localhost:8080

# 5. Start worker (Terminal 2)
go run ./cmd/worker
# Worker will start processing transactions from Redis Streams

# 6. (Optional) Start dashboard (Terminal 3)
make dashboard
# Dashboard will be available at http://localhost:3000
```

### Option 3: Using Make Commands

The project includes a comprehensive Makefile:

```bash
# Start development environment (PostgreSQL + Redis only)
make dev

# Build binaries
make build

# Run API server
make run-api

# Run worker
make run-worker

# Run tests
make test

# Run API test script
make test-api

# View all available commands
make help
```

### First Transaction Flow Example

Let's walk through what happens when you submit your first transaction:

```bash
# 1. Register a user
curl -X POST http://localhost:8080/api/v1/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "test@example.com",
    "password": "SecurePass123",
    "role": "admin"
  }'

# Response: {"token":"eyJhbGci...","expires_in":86400,"user":{...}}

# 2. Create an account
curl -X POST http://localhost:8080/api/v1/accounts \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "<user_id_from_step_1>",
    "account_type": "standard"
  }'

# Response: {"id":"<account_id>","user_id":"...","risk_profile":"low",...}

# 3. Submit a transaction
curl -X POST http://localhost:8080/api/v1/transactions \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{
    "account_id": "<account_id_from_step_2>",
    "amount": 1500.00,
    "currency": "USD",
    "merchant": "Amazon",
    "merchant_category": "retail",
    "location": "New York, NY",
    "country": "US",
    "channel": "online",
    "idempotency_key": "tx-001-$(date +%s)"
  }'

# Response: {
#   "transaction_id": "550e8400-...",
#   "status": "pending",
#   "idempotency_key": "tx-001-...",
#   "created_at": "2026-02-03T10:30:00Z"
# }

# 4. Check transaction status (after ~50-150ms)
curl http://localhost:8080/api/v1/transactions/<transaction_id> \
  -H "Authorization: Bearer <token>"

# Response: {
#   "id": "...",
#   "status": "processed",  // or "flagged" or "blocked"
#   "amount": 1500.00,
#   ...
# }

# 5. Get risk score details
curl http://localhost:8080/api/v1/risk/account/<account_id> \
  -H "Authorization: Bearer <token>"

# Response: {
#   "account_id": "...",
#   "current_risk_level": "low",
#   "avg_transaction_amount": 1500.00,
#   "transaction_count_30d": 1,
#   "flagged_count_30d": 0,
#   ...
# }
```

**What Happened Behind the Scenes:**

1. **API Layer** (0-5ms):
   - Rate limiting check passed
   - JWT token validated
   - Request logged with correlation ID

2. **Ingestion** (5-20ms):
   - Transaction validated
   - Idempotency key checked (no duplicates)
   - Account verified (exists and active)
   - Transaction saved to PostgreSQL with status "pending"
   - Event published to Redis Streams
   - API returns HTTP 201

3. **Scoring** (20-170ms, async):
   - Worker consumes event from Redis Stream
   - Fetches transaction and account details
   - Computes 30+ risk features
   - Applies 15+ scoring rules
   - Calculates behavioral score
   - (Optional) Gets ML score
   - Computes final hybrid score
   - Updates transaction status (processed/flagged/blocked)
   - Saves risk score to database
   - Caches result in Redis

4. **Result**:
   - Transaction status updated
   - Risk score stored with full breakdown
   - Account risk profile updated if needed
   - Analytics metrics updated

### Dashboard

The system includes a sleek, minimalistic dashboard for real-time monitoring:

```bash
# With Docker (recommended) - Dashboard available at http://localhost:3000
docker-compose up -d

# Or serve locally with Python (API must be running on :8080)
make dashboard
```

**Dashboard Features:**
- ğŸ“Š **Real-time System Metrics**: TPS, latency, error rate, queue depth
- ğŸ“ˆ **Risk Distribution**: Visual breakdown of risk levels
- ğŸš© **Flagged Transactions**: View high-risk transactions with rule details
- ğŸ§ª **A/B Testing**: Create and manage scoring experiments
- ğŸ” **Transaction Search**: Search by account, date range, or risk level
- ğŸ“‰ **Trend Analysis**: Historical risk trends and patterns
- ğŸŒ™ **Dark Theme**: Beautiful UI with smooth animations

**Access:**
- URL: http://localhost:3000
- Default login: `admin@example.com` / `admin123` (if seeded)

### Kafka CDC Setup (Optional - for Analytics Pipeline)

To enable the full hybrid architecture with Kafka CDC:

```bash
# Start all services including Kafka ecosystem
make kafka-up

# Wait for services to be ready (~30 seconds)
docker-compose ps

# Set up Debezium CDC connector
make debezium-setup

# Access Kafka UI
make kafka-ui
# Opens http://localhost:8090

# View Kafka worker logs
make kafka-logs
```

**Kafka Services:**
- **Kafka Broker**: localhost:9095
- **Kafka UI**: http://localhost:8090
- **Debezium Connect**: http://localhost:8083

**Note**: Kafka is optional. The system works perfectly with just Redis Streams for scoring. Kafka adds:
- Complete audit trail
- Event replay capability
- ML training data collection
- Data lake synchronization

### Stop All Services

```bash
# Stop all services
docker-compose down

# Stop and remove volumes (clean slate)
docker-compose down -v

# Stop only Kafka services
make kafka-down
```

## ğŸ“¡ API Reference

### Authentication

#### Register User
```bash
POST /api/v1/auth/register
Content-Type: application/json

{
  "email": "user@example.com",
  "password": "SecurePass123",
  "role": "user"
}
```

#### Login
```bash
POST /api/v1/auth/login
Content-Type: application/json

{
  "email": "user@example.com",
  "password": "SecurePass123"
}
```

**Response:**
```json
{
  "token": "eyJhbGciOiJIUzI1NiIs...",
  "expires_in": 86400,
  "user": {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "email": "user@example.com",
    "role": "user"
  }
}
```

### Transactions

#### Ingest Transaction
```bash
POST /api/v1/transactions
Authorization: Bearer <token>
Content-Type: application/json

{
  "account_id": "550e8400-e29b-41d4-a716-446655440000",
  "amount": 1500.00,
  "currency": "USD",
  "merchant": "Amazon",
  "merchant_category": "retail",
  "location": "New York, NY",
  "country": "US",
  "channel": "online",
  "idempotency_key": "tx-unique-key-123"
}
```

#### Batch Ingest
```bash
POST /api/v1/transactions/batch
Authorization: Bearer <token>
Content-Type: application/json

{
  "transactions": [
    { "account_id": "...", "amount": 100, ... },
    { "account_id": "...", "amount": 200, ... }
  ]
}
```

#### Get Flagged Transactions
```bash
GET /api/v1/transactions/flagged?page=1&page_size=20
Authorization: Bearer <token>
```

### Risk Analytics

#### Get Risk Summary
```bash
GET /api/v1/risk/summary?date=2026-02-03
Authorization: Bearer <token>
```

**Response:**
```json
{
  "date": "2026-02-03",
  "total_transactions": 15420,
  "total_amount": 2543210.50,
  "flagged_count": 234,
  "blocked_count": 45,
  "avg_risk_score": 18.5,
  "high_risk_count": 156,
  "critical_risk_count": 45,
  "top_rules_triggered": [
    {"rule_id": "RULE_VELOCITY_BURST", "count": 89},
    {"rule_id": "RULE_SPIKE_ANOMALY", "count": 67}
  ]
}
```

#### Get Account Risk Profile
```bash
GET /api/v1/risk/account/{account_id}
Authorization: Bearer <token>
```

### System Metrics
```bash
GET /api/v1/metrics/system
Authorization: Bearer <token>  # Requires admin/analyst role
```

**Response:**
```json
{
  "timestamp": "2026-02-03T10:30:00Z",
  "transactions_per_sec": 125.5,
  "avg_processing_time_ms": 45.2,
  "queue_depth": 150,
  "active_workers": 5,
  "db_connections_active": 12,
  "db_connections_idle": 13,
  "error_rate": 0.002
}
```

### Backtesting (Event Replay)
```bash
POST /api/v1/backtest/run
Authorization: Bearer <token>  # Requires admin/analyst role
Content-Type: application/json

{
  "account_id": "550e8400-e29b-41d4-a716-446655440000",
  "start_date": "2026-01-01T00:00:00Z",
  "end_date": "2026-02-01T00:00:00Z",
  "sample_size": 100
}
```

**Response:**
```json
{
  "total_transactions": 100,
  "processed_count": 98,
  "failed_count": 2,
  "average_score": 22.5,
  "risk_distribution": {
    "low": 65,
    "medium": 20,
    "high": 10,
    "critical": 3
  },
  "top_triggered_rules": [
    {"rule_id": "RULE_VELOCITY_BURST", "count": 25},
    {"rule_id": "RULE_NEW_LOCATION_HIGH_AMOUNT", "count": 18}
  ],
  "processing_time_ms": 1250,
  "comparison_with_live": {
    "matching_scores": 85,
    "different_scores": 13,
    "avg_score_difference": 2.3,
    "upgraded_risk": 8,
    "downgraded_risk": 5
  }
}
```

### A/B Testing (Experiments)

#### Create Experiment
```bash
POST /api/v1/experiments
Authorization: Bearer <token>  # Requires admin role
Content-Type: application/json

{
  "name": "New Velocity Rules Test",
  "description": "Testing stricter velocity rules",
  "control_rules": ["RULE_VELOCITY_BURST", "RULE_SPIKE_ANOMALY"],
  "test_rules": ["RULE_VELOCITY_BURST", "RULE_SPIKE_ANOMALY", "RULE_RAPID_SMALL_TRANSACTIONS"],
  "traffic_split": 0.2
}
```


#### Start Experiment
```bash
POST /api/v1/experiments/{id}/start
Authorization: Bearer <token>
```

#### Get Experiment Results
```bash
GET /api/v1/experiments/{id}/results
Authorization: Bearer <token>
```

**Response:**
```json
{
  "experiment_id": "abc123",
  "control": {
    "total_transactions": 800,
    "total_amount": 125000.50,
    "avg_risk_score": 18.5,
    "risk_distribution": {"low": 650, "medium": 100, "high": 40, "critical": 10},
    "flagged_count": 40,
    "blocked_count": 10,
    "rules_triggered": {"RULE_VELOCITY_BURST": 45, "RULE_SPIKE_ANOMALY": 30}
  },
  "test": {
    "total_transactions": 200,
    "total_amount": 31250.25,
    "avg_risk_score": 24.2,
    "risk_distribution": {"low": 140, "medium": 35, "high": 18, "critical": 7},
    "flagged_count": 18,
    "blocked_count": 7,
    "rules_triggered": {"RULE_VELOCITY_BURST": 15, "RULE_RAPID_SMALL_TRANSACTIONS": 22}
  },
  "start_time": "2026-02-01T00:00:00Z",
  "last_updated": "2026-02-03T10:30:00Z"
}
```

#### Get Statistical Significance
```bash
GET /api/v1/experiments/{id}/significance
Authorization: Bearer <token>
```

**Response:**
```json
{
  "is_significant": true,
  "confidence_level": 0.95,
  "p_value": 0.023,
  "score_difference": 5.7,
  "score_difference_pct": 30.8,
  "flag_rate_difference": 0.025,
  "sample_size_control": 800,
  "sample_size_test": 200,
  "recommendation": "Test group shows 30.8% higher risk scores. Consider if this aligns with your goals."
}
```

#### Stop/Pause/Delete Experiment
```bash
POST /api/v1/experiments/{id}/stop
POST /api/v1/experiments/{id}/pause
DELETE /api/v1/experiments/{id}
```

## ğŸ§ª Load Testing

Run load tests using k6:

```bash
# Install k6
brew install k6  # macOS
# or: sudo apt install k6  # Ubuntu

# Run smoke test (quick validation)
k6 run scripts/load_test.js

# Run with custom VUs and duration
k6 run --vus 50 --duration 5m scripts/load_test.js

# Run against production
k6 run --env BASE_URL=https://your-api.onrender.com scripts/load_test.js
```

**Test Scenarios:**
| Scenario | VUs | Duration | Purpose |
|----------|-----|----------|---------|
| smoke | 5 | 30s | Quick validation |
| load | 50 | 5m | Normal load testing |
| stress | 100-200 | 10m | Find breaking point |
| spike | 10â†’200â†’10 | 2m | Sudden traffic spike |

## ğŸ“Š Database Schema

### Entity Relationship Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    users     â”‚       â”‚   accounts   â”‚       â”‚    transactions      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚   (partitioned)      â”‚
â”‚ id (PK)      â”‚â”€â”€â”    â”‚ id (PK)      â”‚â”€â”€â”    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ email        â”‚  â”‚    â”‚ user_id (FK) â”‚â—„â”€â”˜    â”‚ id (PK)              â”‚
â”‚ password_hashâ”‚  â””â”€â”€â”€â–ºâ”‚ account_type â”‚       â”‚ account_id (FK)      â”‚â—„â”€â”
â”‚ role         â”‚       â”‚ risk_profile â”‚       â”‚ amount               â”‚  â”‚
â”‚ created_at   â”‚       â”‚ status       â”‚       â”‚ currency             â”‚  â”‚
â”‚ updated_at   â”‚       â”‚ created_at   â”‚       â”‚ merchant             â”‚  â”‚
â”‚ deleted_at   â”‚       â”‚ updated_at   â”‚       â”‚ location             â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ country              â”‚  â”‚
                                              â”‚ channel              â”‚  â”‚
                                              â”‚ status               â”‚  â”‚
                                              â”‚ idempotency_key      â”‚  â”‚
                                              â”‚ created_at           â”‚  â”‚
                                              â”‚ processed_at         â”‚  â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    risk_scores       â”‚       â”‚    audit_logs        â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚ id (PK)              â”‚       â”‚ id (PK)              â”‚                â”‚
â”‚ transaction_id (FK)  â”‚â”€â”€â”€â”€â”€â”€â”€â”‚ event_type           â”‚                â”‚
â”‚ score                â”‚       â”‚ entity_id            â”‚                â”‚
â”‚ risk_level           â”‚       â”‚ entity_type          â”‚                â”‚
â”‚ rules_triggered[]    â”‚       â”‚ user_id (FK)         â”‚                â”‚
â”‚ features (JSONB)     â”‚       â”‚ action               â”‚                â”‚
â”‚ model_version        â”‚       â”‚ payload (JSONB)      â”‚                â”‚
â”‚ processing_time_ms   â”‚       â”‚ ip_address           â”‚                â”‚
â”‚ created_at           â”‚       â”‚ request_id           â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ created_at           â”‚                â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚      rules           â”‚                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                               â”‚
â”‚ id (PK)              â”‚                                               â”‚
â”‚ name                 â”‚                                               â”‚
â”‚ description          â”‚                                               â”‚
â”‚ condition (JSONB)    â”‚                                               â”‚
â”‚ score_impact         â”‚                                               â”‚
â”‚ risk_level           â”‚                                               â”‚
â”‚ priority             â”‚                                               â”‚
â”‚ enabled              â”‚                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
```

### Table Partitioning

Transactions are partitioned by month for optimal query performance:

```sql
transactions_2026_01  -- January 2026
transactions_2026_02  -- February 2026
...
transactions_2026_12  -- December 2026
```

## ğŸ“ˆ Scaling Strategy

### Horizontal Scaling

| Component | Scaling Method | Notes |
|-----------|---------------|-------|
| API Servers | Add replicas behind load balancer | Stateless, scale independently |
| Workers | Add more worker instances | Consumer groups ensure no duplicate processing |
| PostgreSQL | Read replicas for analytics | Write to primary, read from replicas |
| Redis | Redis Cluster for high availability | Streams support consumer groups natively |

### Performance Optimizations

1. **Database**
   - Monthly partitioning on transactions table
   - Composite indexes on (account_id, created_at)
   - Batch inserts for high-throughput ingestion
   - Connection pooling with pgx

2. **Queue Processing**
   - Consumer groups for parallel processing
   - Batch acknowledgment
   - Dead letter queue for failed messages
   - Automatic retry with exponential backoff

3. **Caching**
   - Risk scores cached for 24 hours
   - Account profiles cached for 5 minutes
   - Daily summaries cached (longer for historical data)

### Load Handling

```
Normal Load:    1 API + 1 Worker  â†’  ~100 TPS
Medium Load:    2 API + 3 Workers â†’  ~500 TPS
High Load:      4 API + 10 Workers â†’ ~2000 TPS
```

## ğŸ¯ Design Tradeoffs

### Why Redis Streams over Kafka?

| Factor | Redis Streams | Kafka |
|--------|--------------|-------|
| **Cost** | Free tier friendly (Upstash) | Requires managed service ($100+/mo) |
| **Operational Complexity** | Single binary, minimal config | ZooKeeper/KRaft, partitions, topics |
| **Throughput** | ~10K TPS (sufficient for this scale) | ~100K+ TPS |
| **Consumer Groups** | Native support | Native support |
| **Message Retention** | Memory-bound, configurable | Disk-based, unlimited |

**Decision**: Redis Streams provides Kafka-like semantics (consumer groups, acknowledgments, replay) at zero infrastructure cost. For free-tier deployment, this is the pragmatic choice. Migration to Kafka is straightforward when scale demands it.

### Why Modular Monolith First?

```
Monolith Benefits:
â”œâ”€â”€ Single deployment unit (simpler CI/CD)
â”œâ”€â”€ Shared database transactions
â”œâ”€â”€ No network latency between modules
â”œâ”€â”€ Easier debugging and tracing
â””â”€â”€ Faster development iteration

Microservices Later:
â”œâ”€â”€ When team grows beyond 5-7 engineers
â”œâ”€â”€ When modules need independent scaling
â”œâ”€â”€ When different tech stacks are needed
â””â”€â”€ When deployment independence is critical
```

**Decision**: Start with clear module boundaries (`internal/auth`, `internal/scoring`, etc.) but deploy as one unit. This avoids premature distributed systems complexity while maintaining the option to split later.

### Why Partitioning over Sharding?

| Approach | Partitioning | Sharding |
|----------|-------------|----------|
| **Complexity** | Native PostgreSQL feature | Requires application logic or Citus |
| **Query Routing** | Automatic partition pruning | Manual shard key routing |
| **Transactions** | Full ACID within DB | Distributed transactions needed |
| **Maintenance** | `DETACH PARTITION` for archival | Complex rebalancing |

**Decision**: Time-based partitioning (monthly) handles our access patterns perfectlyâ€”most queries filter by `created_at`. Sharding would be necessary only if single-partition write throughput becomes a bottleneck (unlikely below 50K TPS).

## ğŸ›¡ï¸ Failure Scenarios & Recovery

### Worker Crash Mid-Batch

```
Scenario: Worker processes 50 of 100 messages, then crashes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Messages remain in "pending" state (not acknowledged)   â”‚
â”‚ 2. Redis tracks pending messages per consumer              â”‚
â”‚ 3. Other workers claim abandoned messages after 30s        â”‚
â”‚ 4. XCLAIM moves ownership to healthy worker                â”‚
â”‚ 5. Processing resumes from where it left off               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: Consumer groups + pending entry list (PEL) ensure at-least-once delivery. Idempotency keys prevent duplicate scoring.

### Duplicate Events

```
Scenario: Network glitch causes same transaction to be published twice
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. First event arrives â†’ Transaction created with          â”‚
â”‚    idempotency_key = "tx-abc-123"                          â”‚
â”‚ 2. Second event arrives â†’ INSERT fails (unique constraint) â”‚
â”‚ 3. API returns existing transaction (HTTP 200, not 201)    â”‚
â”‚ 4. No duplicate processing occurs                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: `idempotency_key` column with unique constraint + `ON CONFLICT DO NOTHING` for batch inserts.

### Database Outage

```
Scenario: PostgreSQL becomes unavailable for 2 minutes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server:                                                 â”‚
â”‚ â”œâ”€â”€ Connection pool detects failure                        â”‚
â”‚ â”œâ”€â”€ Returns HTTP 503 (Service Unavailable)                 â”‚
â”‚ â””â”€â”€ Health check fails â†’ Load balancer removes instance    â”‚
â”‚                                                             â”‚
â”‚ Worker:                                                     â”‚
â”‚ â”œâ”€â”€ DB operations fail with timeout                        â”‚
â”‚ â”œâ”€â”€ Messages not acknowledged (remain pending)             â”‚
â”‚ â”œâ”€â”€ Exponential backoff on retries                         â”‚
â”‚ â””â”€â”€ After max retries â†’ Dead letter queue                  â”‚
â”‚                                                             â”‚
â”‚ Recovery:                                                   â”‚
â”‚ â”œâ”€â”€ DB comes back online                                   â”‚
â”‚ â”œâ”€â”€ Connection pool reconnects automatically               â”‚
â”‚ â”œâ”€â”€ Pending messages reprocessed                           â”‚
â”‚ â””â”€â”€ DLQ messages can be replayed manually                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: Connection pooling with health checks, message persistence in Redis, dead letter queue for failed messages.

### Redis Outage

```
Scenario: Redis becomes unavailable
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Impact:                                                     â”‚
â”‚ â”œâ”€â”€ New transactions saved to DB but not queued            â”‚
â”‚ â”œâ”€â”€ Scoring delayed (not lost)                             â”‚
â”‚ â”œâ”€â”€ Cache misses â†’ Direct DB queries (slower)              â”‚
â”‚ â””â”€â”€ Rate limiting falls back to permissive mode            â”‚
â”‚                                                             â”‚
â”‚ Recovery:                                                   â”‚
â”‚ â”œâ”€â”€ Unscored transactions detected via status='pending'    â”‚
â”‚ â”œâ”€â”€ Batch job can requeue pending transactions             â”‚
â”‚ â””â”€â”€ Cache rebuilds on-demand                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protection**: Transactions are persisted to PostgreSQL first, then queued. Redis is used for acceleration, not as source of truth.

## ğŸ” Security Considerations

### Authentication & Authorization

| Layer | Implementation | Details |
|-------|---------------|---------|
| **Password Storage** | bcrypt (cost factor 12) | Resistant to rainbow tables, GPU attacks |
| **Token Format** | JWT (HS256) | Stateless, includes user ID and role |
| **Token Expiration** | 24 hours | Configurable via `JWT_EXPIRATION` |
| **Role-Based Access** | admin, analyst, user | Middleware enforces per-endpoint |

### JWT Security Best Practices

```go
// Current implementation
Token: {
  "user_id": "uuid",
  "email": "user@example.com",
  "role": "user",
  "exp": 1706954400,  // 24h expiration
  "iat": 1706868000   // Issued at
}

// Rotation strategy (recommended for production):
// 1. Short-lived access tokens (15 min)
// 2. Long-lived refresh tokens (7 days)
// 3. Refresh token rotation on use
// 4. Token blacklist for logout/revocation
```

### Rate Limiting

```
Current: Token bucket algorithm
â”œâ”€â”€ 100 requests per minute per IP
â”œâ”€â”€ Automatic cleanup of stale entries
â”œâ”€â”€ Returns 429 with Retry-After header
â””â”€â”€ Protects against:
    â”œâ”€â”€ Brute force login attempts
    â”œâ”€â”€ API abuse / scraping
    â””â”€â”€ DoS attacks (basic)

Production Enhancements:
â”œâ”€â”€ Per-user rate limits (not just IP)
â”œâ”€â”€ Tiered limits by role/plan
â”œâ”€â”€ Distributed rate limiting (Redis)
â””â”€â”€ Adaptive limits based on behavior
```

### Data Protection

| Concern | Mitigation |
|---------|-----------|
| **SQL Injection** | Parameterized queries via pgx (prepared statements) |
| **XSS** | JSON API only, no HTML rendering |
| **CSRF** | Stateless JWT auth, no cookies |
| **Data at Rest** | PostgreSQL encryption (Render managed) |
| **Data in Transit** | TLS enforced (Render provides HTTPS) |
| **PII Handling** | Minimal PII stored, audit logs for access |

### Audit Trail Immutability

```sql
-- Audit logs table design
CREATE TABLE audit_logs (
    id UUID PRIMARY KEY,
    event_type VARCHAR(50) NOT NULL,
    entity_id UUID,
    entity_type VARCHAR(50),
    user_id UUID,
    action VARCHAR(50) NOT NULL,
    payload JSONB,              -- Full event details
    ip_address INET,            -- Client IP
    request_id VARCHAR(100),    -- Correlation ID
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    -- No updated_at, no deleted_at â†’ Immutable
);

-- Protection:
-- 1. No UPDATE/DELETE permissions for application user
-- 2. Append-only table design
-- 3. Consider write-once storage (S3 Glacier) for compliance
-- 4. Cryptographic hashing for tamper detection (future)
```

### Security Checklist for Production

- [ ] Rotate `JWT_SECRET` (use 256-bit random key)
- [ ] Enable PostgreSQL SSL (`sslmode=require`)
- [ ] Set up WAF rules (Cloudflare/AWS WAF)
- [ ] Implement IP allowlisting for admin endpoints
- [ ] Add request signing for inter-service calls
- [ ] Set up security headers (HSTS, CSP, etc.)
- [ ] Regular dependency vulnerability scanning
- [ ] Penetration testing before launch

## â˜ï¸ Deployment

### Render.com (Free Tier)

1. **Create services on Render:**
   - Web Service (API Server)
   - Background Worker (Scoring Engine)
   - PostgreSQL (Managed)
   - Redis (Use Upstash for free tier)

2. **Configure environment variables:**
```
DATABASE_URL=<render-postgres-url>
REDIS_URL=<upstash-redis-url>
JWT_SECRET=<generate-secure-key>
ENVIRONMENT=production
```

3. **Deploy using render.yaml:**
```bash
render blueprint apply
```

### Docker Production

```bash
# Build images
docker build -t risk-engine-api -f Dockerfile.api .
docker build -t risk-engine-worker -f Dockerfile.worker .

# Push to registry
docker push your-registry/risk-engine-api:latest
docker push your-registry/risk-engine-worker:latest
```

## âš™ï¸ Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `PORT` | 8080 | API server port |
| `ENVIRONMENT` | development | Environment (development/production) |
| `DATABASE_URL` | - | PostgreSQL connection string |
| `REDIS_URL` | - | Redis connection string |
| `JWT_SECRET` | - | Secret for JWT signing |
| `JWT_EXPIRATION` | 24h | Token expiration duration |
| `WORKER_CONCURRENCY` | 5 | Number of worker goroutines |
| `WORKER_BATCH_SIZE` | 100 | Messages per batch |

## ğŸ“¡ Observability

The system is designed for enterprise-grade observability and monitoring:

### Structured Logging

All logs are JSON-formatted with correlation IDs for distributed tracing:

```json
{
  "level": "info",
  "time": 1706954400,
  "request_id": "req-abc123",
  "transaction_id": "tx-def456",
  "method": "POST",
  "path": "/api/v1/transactions",
  "status": 201,
  "latency_ms": 145,
  "final_score": 42.5,
  "rule_score": 35.0,
  "behavioral_score": 55.0,
  "scoring_path": "full",
  "rules_triggered": ["RULE_VELOCITY_BURST"],
  "anomalies_detected": ["SPENDING_SPIKE"]
}
```

### Key Metrics Exposed

| Metric | Type | Description |
|--------|------|-------------|
| `transactions_per_sec` | Gauge | Current ingestion rate |
| `avg_processing_time_ms` | Histogram | Scoring latency distribution |
| `queue_depth` | Gauge | Pending messages in Redis Stream |
| `error_rate` | Gauge | Failed transactions / total |
| `db_connections_active` | Gauge | PostgreSQL pool utilization |
| `scoring_path_distribution` | Counter | Fast vs full path breakdown |
| `rules_triggered_total` | Counter | Rule trigger frequency |
| `anomalies_detected_total` | Counter | Anomaly type frequency |

### Alerting Thresholds

```yaml
alerts:
  - name: HighLatency
    condition: avg_processing_time_ms > 500 for 5m
    severity: warning
    
  - name: QueueBacklog
    condition: queue_depth > 1000 for 2m
    severity: critical
    
  - name: HighErrorRate
    condition: error_rate > 0.05 for 5m
    severity: critical
    
  - name: ScoringDrift
    condition: avg_risk_score change > 20% in 1h
    severity: warning
```

### OpenTelemetry Ready

The system is designed for distributed tracing integration:

```go
// Trace context propagation
ctx = otel.GetTextMapPropagator().Extract(ctx, carrier)
span := tracer.Start(ctx, "ScoreTransaction")
defer span.End()

// Span attributes
span.SetAttributes(
    attribute.String("transaction.id", txID),
    attribute.Float64("score.final", finalScore),
    attribute.String("scoring.path", scoringPath),
)
```

**Integration Points:**
- Jaeger / Zipkin for distributed tracing
- Prometheus for metrics scraping
- Grafana for dashboards
- PagerDuty / OpsGenie for alerting

### Health Endpoints

| Endpoint | Purpose |
|----------|---------|
| `GET /health` | Load balancer health check |
| `GET /health/ready` | Kubernetes readiness probe |
| `GET /health/live` | Kubernetes liveness probe |
| `GET /api/v1/metrics/system` | Detailed system metrics (auth required) |

## ğŸ”„ Complete Request/Response Cycle

### Example: Transaction Ingestion with Timing

```
Time    Component              Action                                    Duration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0ms     Client                 HTTP POST /api/v1/transactions
                               
5ms     API Gateway            â€¢ Rate limit check                         2ms
                               â€¢ JWT validation
                               â€¢ Request ID generation
                               
10ms    Ingestion Module       â€¢ Validate payload                        5ms
                               â€¢ Check idempotency key
                               â€¢ Verify account exists
                               
20ms    PostgreSQL             â€¢ INSERT transaction (status: pending)    8ms
                               â€¢ Transaction ID returned
                               
25ms    Redis Streams          â€¢ Publish event to stream                 3ms
                               â€¢ Message ID returned
                               
30ms    API Response           â€¢ HTTP 201 Created                        2ms
                               â€¢ Transaction ID in response
                               â€¢ Status: "pending"
                               
        [ASYNC PROCESSING STARTS]
                               
35ms    Worker                 â€¢ Consume event from Redis Stream         5ms
                               â€¢ Message acknowledged in group
                               
45ms    PostgreSQL             â€¢ SELECT transaction details               8ms
                               â€¢ SELECT account details
                               â€¢ SELECT recent transactions (for features)
                               
60ms    Feature Computation    â€¢ Compute 30+ features                    25ms
                               â€¢ Historical analysis
                               â€¢ Pattern detection
                               
90ms    Rule Engine            â€¢ Evaluate 15+ rules                      15ms
                               â€¢ Calculate rule_score: 35.0
                               
110ms   Behavioral Analysis    â€¢ Z-score calculations                   20ms
                               â€¢ Anomaly detection
                               â€¢ Calculate behavioral_score: 55.0
                               
125ms   ML Scorer              â€¢ ML model inference (if enabled)         15ms
                               â€¢ Calculate ml_score: 48.0
                               
140ms   Final Score            â€¢ Weighted combination                    5ms
                               â€¢ final_score = 42.5
                               â€¢ risk_level = "medium"
                               
150ms   PostgreSQL             â€¢ UPDATE transaction status               8ms
                               â€¢ INSERT risk_score record
                               â€¢ UPDATE account risk profile (if needed)
                               
160ms   Redis Cache            â€¢ Cache risk score (24h TTL)              3ms
                               â€¢ Update account profile cache
                               
165ms   Processing Complete    â€¢ Transaction fully scored
                               â€¢ Status: "processed"
                               â€¢ Available via API
```

**Total Time:**
- **API Response**: ~30ms (transaction created, scoring queued)
- **Scoring Complete**: ~165ms (full processing done)
- **User Experience**: Immediate response, scoring happens async

### Request Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE REQUEST FLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Client Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. API GATEWAY (0-5ms)                                          â”‚
â”‚    â€¢ Rate Limiting: 100 req/min per IP                          â”‚
â”‚    â€¢ JWT Authentication                                         â”‚
â”‚    â€¢ CORS Handling                                              â”‚
â”‚    â€¢ Request ID Generation                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. INGESTION MODULE (5-25ms)                                    â”‚
â”‚    â€¢ Payload Validation                                         â”‚
â”‚    â€¢ Idempotency Check (prevent duplicates)                     â”‚
â”‚    â€¢ Account Verification                                       â”‚
â”‚    â€¢ Transaction Creation (PostgreSQL)                           â”‚
â”‚    â€¢ Event Publishing (Redis Streams)                           â”‚
â”‚    â€¢ Audit Log Creation                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. API RESPONSE (25-30ms)                                       â”‚
â”‚    â€¢ HTTP 201 Created                                           â”‚
â”‚    â€¢ Transaction ID returned                                    â”‚
â”‚    â€¢ Status: "pending"                                          â”‚
â”‚    â€¢ Client can proceed (scoring is async)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ASYNC PATH - Non-blocking]

    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. WORKER CONSUMPTION (30-40ms)                                 â”‚
â”‚    â€¢ Consumer group ensures no duplicate processing             â”‚
â”‚    â€¢ Batch processing (100 messages at a time)                 â”‚
â”‚    â€¢ Message claimed from Redis Stream                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. DATA FETCHING (40-60ms)                                      â”‚
â”‚    â€¢ Load transaction from PostgreSQL                          â”‚
â”‚    â€¢ Load account details                                       â”‚
â”‚    â€¢ Load historical transactions (7d, 30d windows)           â”‚
â”‚    â€¢ Load peer group data (if available)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. FEATURE COMPUTATION (60-90ms)                                 â”‚
â”‚    â€¢ 30+ features computed                                      â”‚
â”‚    â€¢ Spending patterns, velocity, location, etc.               â”‚
â”‚    â€¢ Anomaly detection                                          â”‚
â”‚    â€¢ Peer group comparison                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. SCORING (90-140ms)                                           â”‚
â”‚    â”œâ”€â–º Rule Engine (90-110ms)                                   â”‚
â”‚    â”‚   â€¢ Evaluate 15+ rules                                    â”‚
â”‚    â”‚   â€¢ Calculate rule_score                                   â”‚
â”‚    â”‚                                                             â”‚
â”‚    â”œâ”€â–º Behavioral Analysis (110-130ms)                          â”‚
â”‚    â”‚   â€¢ Z-score calculations                                   â”‚
â”‚    â”‚   â€¢ Calculate behavioral_score                             â”‚
â”‚    â”‚                                                             â”‚
â”‚    â””â”€â–º ML Scorer (130-140ms, optional)                          â”‚
â”‚        â€¢ ML model inference                                      â”‚
â”‚        â€¢ Calculate ml_score                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. FINAL SCORE CALCULATION (140-150ms)                          â”‚
â”‚    â€¢ Weighted combination:                                      â”‚
â”‚      final = 0.50Ã—rule + 0.35Ã—behavioral + 0.15Ã—ml              â”‚
â”‚    â€¢ Risk level determination                                   â”‚
â”‚    â€¢ Transaction status update (processed/flagged/blocked)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. PERSISTENCE (150-165ms)                                      â”‚
â”‚    â€¢ Update transaction status in PostgreSQL                     â”‚
â”‚    â€¢ Save risk_score record                                      â”‚
â”‚    â€¢ Update account risk profile (if escalated)                 â”‚
â”‚    â€¢ Cache results in Redis                                     â”‚
â”‚    â€¢ Acknowledge message (remove from pending)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. COMPLETE (165ms)                                            â”‚
â”‚     â€¢ Transaction fully processed                                â”‚
â”‚     â€¢ Risk score available via API                                   â”‚
â”‚     â€¢ Analytics updated                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› Troubleshooting

### Common Issues and Solutions

#### Issue: Transactions stuck in "pending" status

**Symptoms:**
- Transactions created but never scored
- Status remains "pending" indefinitely

**Diagnosis:**
```bash
# Check if workers are running
docker-compose ps worker

# Check worker logs
docker-compose logs worker

# Check Redis Stream depth
redis-cli XLEN transactions

# Check pending messages
redis-cli XPENDING transactions scoring-workers
```

**Solutions:**
1. **Workers not running**: Start workers with `docker-compose up -d worker`
2. **Redis connection issue**: Check `REDIS_URL` environment variable
3. **Database connection issue**: Check `DATABASE_URL` and connection pool settings
4. **Consumer group not created**: Workers should auto-create, but you can manually create:
   ```bash
   redis-cli XGROUP CREATE transactions scoring-workers 0
   ```

#### Issue: High latency (>500ms)

**Symptoms:**
- Transactions taking too long to score
- API responses slow

**Diagnosis:**
```bash
# Check system metrics
curl http://localhost:8080/api/v1/metrics/system \
  -H "Authorization: Bearer <token>"

# Check database connections
# Look for: db_connections_active, db_connections_idle

# Check queue depth
# Look for: queue_depth (should be < 100)
```

**Solutions:**
1. **Database bottleneck**: Increase connection pool size or add read replicas
2. **Queue backlog**: Scale up workers: `docker-compose --profile scale up -d`
3. **Slow queries**: Check database indexes, use EXPLAIN ANALYZE
4. **Network latency**: Check Redis/PostgreSQL network connectivity

#### Issue: Rate limiting errors (429)

**Symptoms:**
- API returns `429 Too Many Requests`
- `Retry-After` header present

**Solutions:**
1. **Wait**: Rate limit is 100 requests/minute per IP
2. **Distribute load**: Use multiple IPs or implement client-side rate limiting
3. **Adjust limit**: Modify rate limiter configuration in code (not recommended for production)

#### Issue: Duplicate transactions

**Symptoms:**
- Same transaction processed multiple times
- Duplicate risk scores

**Solutions:**
1. **Use idempotency keys**: Always provide unique `idempotency_key` per transaction
2. **Check idempotency**: System should return existing transaction if key matches
3. **Verify uniqueness**: Ensure idempotency keys are truly unique

#### Issue: Workers not processing messages

**Symptoms:**
- Messages accumulating in Redis Stream
- No worker activity in logs

**Diagnosis:**
```bash
# Check consumer group status
redis-cli XINFO GROUPS transactions

# Check pending messages
redis-cli XPENDING transactions scoring-workers

# Check worker logs
docker-compose logs -f worker
```

**Solutions:**
1. **Restart workers**: `docker-compose restart worker`
2. **Check consumer group**: Ensure group name matches in config
3. **Check Redis connectivity**: Verify `REDIS_URL` is correct
4. **Claim pending messages**: Workers should auto-claim, but you can manually:
   ```bash
   # Workers will claim messages after 30s idle time
   ```

#### Issue: Database connection errors

**Symptoms:**
- `connection refused` errors
- `too many connections` errors

**Solutions:**
1. **Check PostgreSQL is running**: `docker-compose ps postgres`
2. **Check connection string**: Verify `DATABASE_URL` format
3. **Reduce connection pool**: Lower `DB_MAX_OPEN_CONNS` if hitting limits
4. **Check PostgreSQL max connections**: Default is 100, may need to increase

#### Issue: Cache misses

**Symptoms:**
- Slow API responses for risk scores
- Direct database queries instead of cache

**Solutions:**
1. **Check Redis connectivity**: Verify `REDIS_URL`
2. **Check cache TTL**: Risk scores cached for 24h, profiles for 5m
3. **Monitor cache hit rate**: Should be > 70% for risk scores
4. **Warm cache**: Pre-load frequently accessed data

### Debugging Commands

```bash
# View all service logs
docker-compose logs -f

# View specific service logs
docker-compose logs -f api-server
docker-compose logs -f worker

# Check service health
curl http://localhost:8080/health

# Check system metrics
curl http://localhost:8080/api/v1/metrics/system \
  -H "Authorization: Bearer <token>"

# Check Redis Stream info
redis-cli XINFO STREAM transactions

# Check database connections
docker exec risk-engine-postgres psql -U postgres -d risk_engine \
  -c "SELECT count(*) FROM pg_stat_activity;"

# Check recent transactions
docker exec risk-engine-postgres psql -U postgres -d risk_engine \
  -c "SELECT id, status, created_at FROM transactions ORDER BY created_at DESC LIMIT 10;"

# Check risk scores
docker exec risk-engine-postgres psql -U postgres -d risk_engine \
  -c "SELECT transaction_id, score, risk_level, created_at FROM risk_scores ORDER BY created_at DESC LIMIT 10;"
```

### Performance Tuning

**For Higher Throughput:**
1. Increase worker concurrency: `WORKER_CONCURRENCY=10`
2. Increase batch size: `WORKER_BATCH_SIZE=200`
3. Scale workers: `docker-compose --profile scale up -d`
4. Optimize database queries (add indexes)
5. Use connection pooling effectively

**For Lower Latency:**
1. Enable fast-path scoring (automatic for low-risk)
2. Increase Redis cache hit rate
3. Use read replicas for analytics queries
4. Optimize feature computation (cache historical data)
5. Reduce database round trips (batch queries)

## ğŸ”® Future AWS Migration Path

| Current | AWS Equivalent | Benefits |
|---------|---------------|----------|
| Redis Streams | Amazon MSK (Kafka) | Higher throughput, better durability |
| Render PostgreSQL | Amazon RDS | Multi-AZ, automated backups, read replicas |
| Docker on Render | Amazon ECS/EKS | Auto-scaling, better orchestration |
| Basic Metrics | Amazon CloudWatch + Prometheus | Comprehensive observability |
| Manual Scaling | AWS Auto Scaling | Automatic scaling based on metrics |
| Behavioral Scoring | Amazon SageMaker | Advanced ML models, training pipelines |
| Rate Limiting (in-memory) | AWS API Gateway | Distributed rate limiting, WAF integration |

**Migration Strategy:**
1. **Phase 1**: Move to RDS (database migration)
2. **Phase 2**: Replace Redis Streams with MSK (message queue migration)
3. **Phase 3**: Deploy to ECS/EKS (container orchestration)
4. **Phase 4**: Integrate CloudWatch/Prometheus (observability)
5. **Phase 5**: Connect to SageMaker (ML integration)

## ğŸ“š Additional Resources

- **Architecture Details**: See [docs/architecture.md](docs/architecture.md)
- **Data Model**: See [docs/data-model.md](docs/data-model.md)
- **Scaling Strategy**: See [docs/scaling-strategy.md](docs/scaling-strategy.md)
- **API Examples**: See [scripts/test_api.sh](scripts/test_api.sh)
- **Load Testing**: See [scripts/load_test.js](scripts/load_test.js)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

**Development Guidelines:**
- Follow Go best practices and conventions
- Add tests for new features
- Update documentation
- Ensure backward compatibility
- Run `make test` before submitting PR

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

**Built with â¤ï¸ for enterprise-grade risk management**

*For questions, issues, or contributions, please open an issue on GitHub.*
